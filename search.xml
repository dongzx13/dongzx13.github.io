<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[word2vec]]></title>
    <url>%2F2018%2F11%2F27%2Fword2vec%2F</url>
    <content type="text"><![CDATA[word2vec 总结 wordmeaning 用dot product计算similartiy distributional similarity 和 distributed 概念区别 distributional similarity 是可以用这个词所搭配的前后文词语来了解这个词的含义 对比的是denotational的对词下定义的方式，也就是字典上解释这个词所用的方式，而上面所说的更像是我们用例句的方式来学习这个词 distributed是与one-hot vector相反的对word的表示方式，是dense vector]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch-faster-rcnn]]></title>
    <url>%2F2018%2F10%2F19%2Fpytorch-faster-rcnn%2F</url>
    <content type="text"><![CDATA[Image Pre-Processing&lt;!–more&gt; The following pre-processing steps are applied to an image before it is sent through the network. These steps must be identical for both training and inference. The mean vector (, one number corresponding to each color channel) is not the mean of the pixel values in the current image but a configuration value that is identical across all training and test images. The default values for and parameters are 600 and 1200 respectively. Network OrganizationA R-CNN uses neural networks to solve two main problems: Identify promising regions (Region of Interest – ROI) in an input image that are likely to contain foreground objects Compute the object class probability distribution of each ROI – i.e., compute the probability that the ROI contains an object of a certain class. The user can then select the object class with the highest probability as the classification result. R-CNNs consist of three main types of networks: Head Region Proposal Network (RPN) Classification Network]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>faster-rcnn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Median of two sorted Arrays]]></title>
    <url>%2F2018%2F10%2F15%2FMedian-of-two-sorted-Arrays%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[dynamic connectivity]]></title>
    <url>%2F2018%2F09%2F12%2Fdynamic-connectivity%2F</url>
    <content type="text"><![CDATA[Quick-findSolve the dynamic connectivity problem -&gt; Quick Find so called eager algorithm贪心算法]]></content>
  </entry>
  <entry>
    <title><![CDATA[test_time_caffe]]></title>
    <url>%2F2018%2F09%2F05%2Ftest-time-caffe%2F</url>
    <content type="text"><![CDATA[Test time12345678#!/usr/bin/env shnow=$(date +"%Y%m%d_%H%M%S")log_name="test___"$now""srun -p Test --exclusive --gres=gpu:1 \ /mnt/lustre/share/sensephoto/caffes/caffe/build/tools/caffe time \ --model=$1 \ --iterations=1000 \ 2&gt;&amp;1|tee $log_name.log &amp;]]></content>
      <categories>
        <category>caffe</category>
      </categories>
      <tags>
        <tag>caffe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[stereo camera]]></title>
    <url>%2F2018%2F08%2F22%2Fstereo-camera%2F</url>
    <content type="text"><![CDATA[Stereo vision basicspinhole camera model]]></content>
      <tags>
        <tag>stereo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Depth-camera]]></title>
    <url>%2F2018%2F08%2F14%2FDepth-camera%2F</url>
    <content type="text"><![CDATA[立体视觉的基本知识 stereoscopic vision ##Why depth? 深度相机是添加一个全新的信息通道，每个像素的距离。 ##立体视觉 立体视觉的深度信息提取是受人类双目视觉的启发。它依赖于两个平行的视觉窗口，并通过估计左右图像中匹配的关键点的差异来计算深度。]]></content>
      <tags>
        <tag>Depth-camera</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[librealsense]]></title>
    <url>%2F2018%2F08%2F01%2Flibrealsense%2F</url>
    <content type="text"><![CDATA[Depth from Stereo Using this tutorial you will learn the basics of stereoscopic vision, including block-matching, calibration and rectification, depth from stereo using opencv, passive vs. active stereo and relation to structured light. Why Depth?Regular consumer web-cams offer streams of RGB data within the visible spectrum. This data can be used for object recognition and tracking, as well as some basic scene understanding. Even with machine learning grasping the exact dimensions of physical objects is a very hard problem This is where depth cameras come-in. The goal of a depth camera is to add a brand-new channel of information, with distance to every pixel.This new channel can be used just like the rest (for training and image processing) but also for measurement and scene reconstruction. n VisionDepth from Stereo is a classic computer vision algorithm inspired by human binocular vision system. It relies on two parallel view-ports and calculates depth by estimating disparities between matching key-points in the left and right images: Depth from Stereo algorithm finds disparity by matching blocks in left and right images Most naive implementation of this idea is the SSD (Sum of Squared Differences) block-matching algorithm: 123456789101112131415161718192021222324import numpyfx = 942.8 # lense focal lengthbaseline = 54.8 # distance in mm between the two camerasdisparities = 64 # num of disparities to considerblock = 15 # block size to matchunits = 0.001 # depth unitsfor i in xrange(block, left.shape[0] - block - 1): for j in xrange(block + disparities, left.shape[1] - block - 1): ssd = numpy.empty([disparities, 1]) # calc SSD at all possible disparities l = left[(i - block):(i + block), (j - block):(j + block)] for d in xrange(0, disparities): r = right[(i - block):(i + block), (j - d - block):(j - d + block)] ssd[d] = numpy.sum((l[:,:]-r[:,:])**2) # select the best match disparity[i, j] = numpy.argmin(ssd)# Convert disparity to depthdepth = np.zeros(shape=left.shape).astype(float)depth[disparity &gt; 0] = (fx * baseline) / (units * disparity[disparity &gt; 0]) Rectified image pair used as input to the algorithm Depth map produced by the naive SSD block-matching implementation Point-cloud reconstructed using SSD block-matching There are several challenges that any actual product has to overcome: Ensuring that the images are in fact coming from two parallel views Filtering out bad pixels where matching failed due to occlusion Expanding the range of generated disparities from fixed set of integers to achieve sub-pixel accuracy D435This document describes the projection mathematics relating the images provided by the RealSense depth devices to their associated 3D coordinate systems, as well as the relationships between those coordinate systems. These facilities are mathematically equivalent to those provided by previous APIs and SDKs, but may use slightly different phrasing of coefficients and formulas. Table of Contents Pixel Coordinates Point Coordinates Intrinsic Camera Parameters Distortion Models Extrinsic Camera Parameters Depth Image Formats Processing Blocks Helpers Point Cloud Frame Alignment Appendix: Model Specific Details SR300 D400 Pixel CoordinatesEach stream of images provided by this SDK is associated with a separate 2D coordinate space, specified in pixels, with the coordinate [0,0] referring to the center of the top left pixel in the image, and [w-1,h-1] referring to the center of the bottom right pixel in an image containing exactly w columns and h rows. That is, from the perspective of the camera, the x-axis points to the right and the y-axis points down. Coordinates within this space are referred to as “pixel coordinates”, and are used to index into images to find the content of particular pixels. 左上角是[0,0],右下角是[w-1,h-1] Point CoordinatesEach stream of images provided by this SDK is also associated with a separate 3D coordinate space, specified in meters, with the coordinate [0,0,0] referring to the center of the physical imager. Within this space, the positive x-axis points to the right, the positive y-axis points down, and the positive z-axis points forward. Coordinates within this space are referred to as “points”, and are used to describe locations within 3D space that might be visible within a particular image. Intrinsic Camera ParametersThe relationship between a stream’s 2D and 3D coordinate systems is described by its intrinsic camera parameters, contained in the rs2_intrinsics struct. Each model of RealSense device is somewhat different, and the rs2_intrinsics struct must be capable of describing the images produced by all of them. The basic set of assumptions is described below: Images may be of arbitrary size The width and height fields describe the number of rows and columns in the image, respectively The field of view of an image may vary The fx and fy fields describe the focal length of the image, as a multiple of pixel width and height The pixels of an image are not necessarily square The fx and fy fields are allowed to be different (though they are commonly close) The center of projection is not necessarily the center of the image The ppx and ppy fields describe the pixel coordinates of the principal point (center of projection) The image may contain distortion The model field describes which of several supported distortion models was used to calibrate the image, and the coeffs field provides an array of up to five coefficients describing the distortion model Knowing the intrinsic camera parameters of an images allows you to carry out two fundamental mapping operations. Projection Projection takes a point from a stream’s 3D coordinate space, and maps it to a 2D pixel location on that stream’s images. It is provided by the header-only function rs2_project_point_to_pixel(...). Deprojection Deprojection takes a 2D pixel location on a stream’s images, as well as a depth, specified in meters, and maps it to a 3D point location within the stream’s associated 3D coordinate space. It is provided by the header-only function rs2_deproject_pixel_to_point(...). Intrinsic parameters can be retrieved from any rs2::video_stream_profile object via a call to get_intrinsics(). (See example here) Distortion ModelsBased on the design of each model of RealSense device, the different streams may be exposed via different distortion models. None An image has no distortion, as though produced by an idealized pinhole camera. This is typically the result of some hardware or software algorithm undistorting an image produced by a physical imager, but may simply indicate that the image was derived from some other image or images which were already undistorted. Images with no distortion have closed-form formulas for both projection and deprojection, and can be used with both rs2_project_point_to_pixel(...) and rs2_deproject_pixel_to_point(...). Modified Brown-Conrady Distortion An image is distorted, and has been calibrated according to a variation of the Brown-Conrady Distortion model. This model provides a closed-form formula to map from undistorted points to distorted points, while mapping in the other direction requires iteration or lookup tables. Therefore, images with Modified Brown-Conrady Distortion are being undistorted when calling rs2_project_point_to_pixel(...). This model is used by the RealSense D415’s color image stream. Inverse Brown-Conrady Distortion An image is distorted, and has been calibrated according to the inverse of the Brown-Conrady Distortion model. This model provides a closed-form formula to map from distorted points to undistored points, while mapping in the other direction requires iteration or lookup tables. Therefore, images with Inverse Brown-Conrady Distortion are being undistorted when calling rs2_deproject_pixel_to_point(...). This model is used by the RealSense SR300’s depth and infrared image streams. Although it is inconvenient that projection and deprojection cannot always be applied to an image, the inconvenience is minimized by the fact that RealSense devices always support deprojection from depth images, and always support projection to color images. Therefore, it is always possible to map a depth image into a set of 3D points (a point cloud), and it is always possible to discover where a 3D object would appear on the color image. Extrinsic Camera ParametersThe 3D coordinate systems of each stream may in general be distinct. For instance, it is common for depth to be generated from one or more infrared imagers, while the color stream is provided by a separate color imager. The relationship between the separate 3D coordinate systems of separate streams is described by their extrinsic parameters, contained in the rs2_extrinsics struct. The basic set of assumptions is described below: Imagers may be in separate locations, but are rigidly mounted on the same physical device The translation field contains the 3D translation between the imager’s physical positions, specified in meters Imagers may be oriented differently, but are rigidly mounted on the same physical device The rotation field contains a 3x3 orthonormal rotation matrix between the imager’s physical orientations All 3D coordinate systems are specified in meters There is no need for any sort of scaling in the transformation between two coordinate systems All coordinate systems are right handed and have an orthogonal basis There is no need for any sort of mirroring/skewing in the transformation between two coordinate systems Knowing the extrinsic parameters between two streams allows you to transform points from one coordinate space to another, which can be done by calling rs2_transform_point_to_point(...). This operation is defined as a standard affine transformation using a 3x3 rotation matrix and a 3-component translation vector. Extrinsic parameters can be retrieved via a call to rs2_get_extrinsics(...) between any two streams which are supported by the device, or using a rs2::stream_profile object via get_extrinsics_to(...) (see example here). One does not need to enable any streams beforehand, the device extrinsics are assumed to be independent of the content of the streams’ images and constant for a given device for the lifetime of the program. Depth Image FormatsAs mentioned above, mapping from 2D pixel coordinates to 3D point coordinates via the rs2_intrinsics structure and the rs2_deproject_pixel_to_point(...) function requires knowledge of the depth of that pixel in meters. Certain pixel formats, exposed by this SDK, contain per-pixel depth information, and can be immediately used with this function. Other images do not contain per-pixel depth information, and thus would typically be projected into instead of deprojected from. RS2_FORMAT_Z16 (Under rs2_format) Depth is stored as one unsigned 16-bit integer per pixel, mapped linearly to depth in camera-specific units. The distance, in meters, corresponding to one integer increment in depth values can be queried via rs2_get_depth_scale(...) or using a rs2::depth_sensor via get_depth_scale() (see example here). The following shows how to retrieve the depth of a pixel in meters: Using C API: 123const float scale = rs2_get_depth_scale(sensor, NULL);const uint16_t * image = (const uint16_t *)rs2_get_frame_data(frame, NULL);float depth_in_meters = scale * image[pixel_index]; Using C++ API: 12rs2::depth_frame dpt_frame = frame.as&lt;rs2::depth_frame&gt;();float pixel_distance_in_meters = dpt_frame.get_distance(x,y); using Python API: 12dpt_frame = pipe.wait_for_frames().get_depth_frame().as_depth_frame()pixel_distance_in_meters = dpt_frame.get_distance(x,y) If a device fails to determine the depth of a given image pixel, a value of zero will be stored in the depth image. This is a reasonable sentinel for “no depth” because all pixels with a depth of zero would correspond to the same physical location, the location of the imager itself. The default scale of an SR300 device is 1/32th of a millimeter, allowing for a maximum expressive range of two meters. However, the scale is encoded into the camera’s calibration information, potentially allowing for long-range models to use a different scaling factor. The default scale of a D400 device is one millimeter, allowing for a maximum expressive range of ~65 meters. The depth scale can be modified by calling rs2_set_option(...) with RS2_OPTION_DEPTH_UNITS, which specifies the number of meters per one increment of depth. 0.001 would indicate millimeter scale, while 0.01 would indicate centimeter scale. Processing Blocks HelpersThe SDK provides two main processing blocks related to image projecting: Point Cloud Frame Alignment Point CloudAs part of the API we offer a processing block for creating a point cloud and corresponding texture mapping from depth and color frames. The point cloud created from a depth image is a set of points in the 3D coordinate system of the depth stream. The following demonstrates how to create a point cloud object: Using C API: 123456789rs2_frame_queue* q = rs2_create_frame_queue(1, NULL);rs2_processing_block* pc = rs2_create_pointcloud(NULL);rs2_start_processing_queue(pc, q, NULL);rs2_process_frame(pc, depth_frame, NULL);rs2_frame* points = rs2_wait_for_frame(q, 5000, NULL);const rs2_stream_profile* sp = rs2_get_frame_stream_profile(color_frame, NULL);rs2_get_stream_profile_data(sp, ..., &amp;uid, ..., NULL);rs2_set_option((rs2_options*)pc, RS2_OPTION_TEXTURE_SOURCE, uid, NULL);rs2_process_frame(pc, color_frame, NULL); Using C++ API: 123rs2::pointcloud pc;rs2::points points = pc.calculate(depth_frame);pc.map_to(color_frame); using Python API: 1234import pyrealsense2 as rspc = rs.pointcloud()points = pc.calculate(depth_frame)pc.map_to(color_frame) For additional examples, see examples/pointcloud. In addition, an example for integrating with PCL (Point Cloud Library) is available here. Frame AlignmentUsually when dealing with color and depth images, mapping each pixel from one image to the other is desired. The SDK offers a processing block for aligning the image to one another, producing a set of frames that share the same resolution and allow for easy mapping of pixels. The following demonstrates how to create an align object: Using C API: 123456789rs2_processing_block* align = rs2_create_align(RS2_STREAM_COLOR, NULL);rs2_frame_queue* q = rs2_create_frame_queue(1, NULL);rs2_start_processing_queue(align, q, NULL);rs2_process_frame(align, depth_and_color_frameset, NULL);rs2_frame* aligned_frames = rs2_wait_for_frame(q, 5000, NULL);for (i = 0; i &lt; rs2_get_frame_points_count(aligned_frames, NULL); i++)&#123; rs2_frame* aligned_frame = rs2_extract_frame(aligned_frames, i, NULL);&#125; Using C++ API: 1234rs2::align align(RS2_STREAM_COLOR);auto aligned_frames = align.process(depth_and_color_frameset);rs2::video_frame color_frame = aligned_frames.first(RS2_STREAM_COLOR);rs2::depth_frame aligned_depth_frame = aligned_frames.get_depth_frame(); using Python API: 12345import pyrealsense2 as rsalign = rs.align(rs.stream.color)aligned_frames = align.proccess(depth_and_color_frameset)color_frame = aligned_frames.first(rs.stream.color)aligned_depth_frame = aligned_frames.get_depth_frame() For additional examples, see examples/align and align-depth2color.py. Appendix: Model Specific DetailsIt is not necessary to know which model of RealSense device is plugged in to successfully make use of the projection capabilities of this SDK. However, developers can take advantage of certain known properties of given devices. SR300 Depth images are always pixel-aligned with infrared images The depth and infrared images have identical intrinsics The depth and infrared images will always use the Inverse Brown-Conrady distortion model The extrinsic transformation between depth and infrared is the identity transform Pixel coordinates can be used interchangeably between these two streams Color images have no distortion When projecting to the color image on these devices, the distortion step can be skipped entirely D400-Series Left and right infrared images are rectified by default (Y16 format is not) The two infrared streams have identical intrinsics The two infrared streams have no distortion There is no rotation between left and right infrared images (identity matrix) There is translation on only one axis between left and right infrared images (translation[1] and translation[2] are zero) Therefore, the y component of pixel coordinates can be used interchangeably between these two streams Stereo disparity is related to depth via an inverse linear relationship, and the distance of a point which registers a disparity of 1 can be queried via 1 / rs2_get_depth_scale(...). The following shows how to retrieve the depth of a pixel in meters: 123const float scale = rs2_get_depth_scale(sensor, NULL);const uint16_t * image = (const uint16_t *)rs2_get_frame_data(frame, NULL);float depth_in_meters = 1 / (scale * image[pixel_index]); Unlike RS2_FORMAT_Z16, a disparity value of zero is meaningful. A stereo match with zero disparity will occur for objects “at infinity”, objects which are so far away that the parallax between the two imagers is negligible. By contrast, there is a maximum possible disparity. Disparity is currently only available on the D400. Controlling disparity can be modified using the advanced_mode controls.]]></content>
      <categories>
        <category>openark</category>
      </categories>
      <tags>
        <tag>realsense open_ark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenCv_learn]]></title>
    <url>%2F2018%2F07%2F30%2FOpenCv-learn%2F</url>
    <content type="text"><![CDATA[Mat的理解 Mat的好处是可以自动的释放内存 Mat是一个基础的类有两个部分 矩阵头（包括矩阵大小，存储方式，存储地址等信息） 能指向存储像素信息的矩阵 Mat is basically a class with two data parts: the matrix header (containing information such as the size of the matrix, the method used for storing, at which address is the matrix stored, and so on) and a pointer to the matrix containing the pixel values (taking any dimensionality depending on the method chosen for storing)? opencv考虑减少对大图片不必要的拷贝，opencv使用reference counting system。每一个mat对象都有自己的矩阵头，但是实际的图片矩阵可以被多个对象的指针共享，即多个指针指向同一个地址。copy操作只会拷贝矩阵头和指向的指针不会拷贝数据本身。 123456Mat A, C; // creates just the header partsA = imread(argv[1], IMREAD_COLOR); // here we'll know the method used (allocate matrix)Mat B(A); // Use the copy constructorC = A; // Assignment operator A，B，C的header是不同的，但是指向的是同一块内存，通过指针修改内存的数值会导致三个的存储的值改变。 可以通过建立新的hearder来产生ROI。 12Mat D (A, Rect(10, 10, 100, 100) ); // using a rectangleMat E = A(Range::all(), Range(1,3)); // using row and column boundaries 但所有的对象都不用了之后，内存就会被释放。利用的reference counting mechanism。 如果希望拷贝数据本身的话，opencv提供clone()和copyTo()这两个函数 123Mat F = A.clone();Mat G;A.copyTo(G); Summary: Output image allocation for OpenCV functions is automatic (unless specified otherwise). You do not need to think about memory management with OpenCVs C++ interface. The assignment operator and the copy constructor only copies the header. The underlying matrix of an image may be copied using the clone() and copyTo() functions.]]></content>
      <categories>
        <category>openark</category>
      </categories>
      <tags>
        <tag>C++, openark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Const_C++]]></title>
    <url>%2F2018%2F07%2F30%2FConst-C%2F</url>
    <content type="text"><![CDATA[Const c++的使用总结 12345const std::vector&lt;Hand::Ptr&gt; &amp; HandDetector::getHands() const &#123; return hands; &#125;//const函数重载 基本知识 const修饰常量时，定义时必须初始化 对于类中的const成员必须通过初始化列表进行初始化 123456789101112131415class A&#123; public: A(int i); void print(); const int &amp;r; private: const int a; static const int b;&#125;;const int A::b=10;A::A(int i):a(i),r(a)&#123; &#125; Const 默认在文件中为局部变量，如果需要在别的文件中访问需要显示的定义为外部变量，非const的变量默认为外部变量 很有趣的地方： 1234567891011#include &lt;iostream&gt;int main(int argc, const char * argv[]) &#123; // insert code here... double a = 3.14; const int &amp;ri = a; // Non-const lvalue reference to type 'int' cannot bind to a value of unrelated type 'double' int &amp;b = a; std::cout &lt;&lt; "Hello, World!\n"; return 0;&#125; 非const引用只能绑定到与该引用相同类型的对象。 const引用则可以绑定到不同但相关的类型的对象或绑定到右值。 原因在于编译器在编译的时候，对于引用存放的是一个对象的地址，对于不可寻址的值，如文字常量以及不同类型的对象，编译器为了实现引用，必须生成一个临时对象，引用实际上指向该对象，但用户不能访问它。ri是const所以没有办法修改的ri的赋值，但是对于b一旦修改了b的值我们希望同时修改a的值，但是由于编译器其实修改的是临时对象temp的值，而不是a的值，所以会造成误解，所以在编译的时候回报错。 const对象的动态数组 1234const int *temp = new const int[100]();//const的修饰的数组必须初始化，所以使用这种方式可以zero initial整个数组const std::string *temp = new std::string[2];//会调用string类的默认构造函数初始化//c++允许定义类类型的const数组，但是必须提供默认构造函数 指针和const修饰符 指向const的指针123456789101112const double *cptr;// cptr是一个指向double类型的const对象的指针，cptr可以指向不同的const对象，但是不能通过cptr修改对象的值。*ctpn = 42;//错误，不能修改值const double pi = 3.14；double *ptr = $pi; //错误，不能把一个const的对象的地址赋给一个普通的非const指针，因为这样可以修改const的值const double *ctpr = &amp;pi;//可以的//允许把非const对象的值赋给const对象的指针 但是不能用const对象的指针来修改其值，修改方法需要借助别的非const指针double dval = 3.13;cptr = &amp;dval;double *ptr = &amp;dval;*ptr =3.14;cout&lt;&lt; *cptr;//结果是3.14 自以为指向const的指针，偷偷修改它也不知道 常指针 本身的值不能修改，与其他const量一样，需要在定义的时候初始化。 12int a = 0;int *const ptr = &amp;a;//能不能修改指向对象的值，取决于指向对象本身的类型 函数和const限定符的关系 类中的const成员函数 在一个类中，任何不会修改数据成员的函数都应该声明为const类型。如果在编写const成员函数时，不慎修改了数据成员，或者调用了其它非const成员函数，编译器将指出错误，这无疑会提高程序的健壮性。使用const关键字进行说明的成员函数，称为常成员函数。只有常成员函数才有资格操作常量或常对象，没有使用const关键字说明的成员函数不能用来操作常对象。常成员函数说明格式如下： &lt;类型说明符&gt; &lt;函数名&gt; (&lt;参数表&gt;) const； 12345678910111213141516class tempstack&#123;private: int m_num; int m_data[100];public: void Push(int elem); int Pop(void); int GetCount(void) const;&#125;;int tempstack::GetCount(void) const&#123; //++m_num;//编译错误Cannot assign to non-static data member within const member function 'GetCount' //Pop();//Member function 'Pop' not viable: 'this' argument has type 'const tempstack', but function is not marked const return m_num;&#125; 函数重载 123456789101112131415161718192021222324252627282930#include &lt;iostream&gt;class R&#123;public: R(int r1,int r2) &#123; R1 = r1; R2 = r2; &#125; void print(); void print() const;private: int R1,R2;&#125;;void R::print()&#123; std::cout&lt;&lt;R1;&#125;void R::print() const&#123; std::cout&lt;&lt;R2;&#125;int main(int argc, const char * argv[]) &#123; R a(1,2); a.print();//1 const R b(3,4); b.print();//4 return 0;&#125; const 对象默认调用const成员函数]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[handDetect]]></title>
    <url>%2F2018%2F07%2F27%2FhandDetect%2F</url>
    <content type="text"><![CDATA[使用RealSense进行手势识别 原理 Compute a depth map The depth map is constructed by analyzing a speckle pattern of infrared laser light 具体建立结构光的原理是受专利保护的 Structured light general principle: project a known pattern onto the scene and infer depth from the deformation of that pattern The Kinect combines structured light with two classic computer vision techniques: depth from focus, and depth from stereo Depth from focus uses the principle that stuff that is more blurry is further away 越远的物体就会更模糊 The astigmatic lens causes a projected circle to become an ellipse whose orientation depends on depth 像散透镜使投影圆变成椭圆，其取向取决于深度 Depth from stereo uses parallax 2.infer body postion 代码梳理12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061// C++ Libraries#include &lt;stdio.h&gt;#include &lt;iostream&gt;#include &lt;string&gt;// OpenCV Libraries#include &lt;opencv/cxcore.h&gt;#include "opencv2/highgui/highgui.hpp"// OpenARK Libraries#include "Core.h"#include "RS2Camera.h"#include "Visualizer.h"using namespace ark;int main() &#123; // change the SR300Camera to other type if needed ark::DepthCamera::Ptr camera = std::make_shared&lt;ark::RS2Camera&gt;(); ark::DetectionParams::Ptr params = ark::DetectionParams::create(); //ark::HandDetector handDetector(true, params); PlaneDetector::Ptr planeDetector = std::make_shared&lt;PlaneDetector&gt;(); HandDetector::Ptr handDetector = std::make_shared&lt;HandDetector&gt;(planeDetector); //ark::HandDetector handDetector(false, params); // change first argument to true to remove planes in frame camera-&gt;beginCapture(); //1.step int frame = 0; while (true) &#123; // Show image cv::Mat xyzVisual; ark::Visualizer::visualizeXYZMap(camera-&gt;getXYZMap(), xyzVisual); cv::imshow("XYZ Map", xyzVisual); // Find hands handDetector-&gt;update(*camera); auto hands = handDetector-&gt;getHands(); if (!hands.empty()) &#123; ark::Hand::Ptr hand = hands[0]; // Show the hand cv::Mat visual = camera-&gt;getXYZMap().clone(); ark::Visualizer::visualizeHand(visual, visual, hand.get(), hand-&gt;getSVMConfidence()); cv::imshow("Result", visual); &#125; /**** Start: Loop Break Condition ****/ int c = cv::waitKey(1); if (c == 'q' || c == 'Q' || c == 27) &#123; break; &#125; /**** End: Loop Break Condition ****/ ++frame; &#125; return 0;&#125; 和硬件连接的部分capture1234567891011121314151617181920 /** * Begin capturing frames continuously from this camera on a parallel thread, * capped at a certain maximum FPS. * WARNING: throws an error if capture already started. * @param fps_cap maximum FPS of capture (-1 to disable) * @param removeNoise if true, performs noise removal on the depth image after retrieving it * @see endCapture * @see isCapturing */ void beginCapture(int fps_cap = -1, bool remove_noise = true);void DepthCamera::beginCapture(int fps_cap, bool remove_noise)&#123; ASSERT(captureInterrupt == true, "beginCapture: already capturing from this camera"); captureInterrupt = false; std::thread thd(&amp;DepthCamera::captureThreadingHelper, this, fps_cap, &amp;captureInterrupt, remove_noise); thd.detach();&#125; c++多线程 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960// C++ Libraries#include &lt;stdio.h&gt;#include &lt;iostream&gt;#include &lt;string&gt;// OpenCV Libraries#include &lt;opencv/cxcore.h&gt;#include &quot;opencv2/highgui/highgui.hpp&quot;// OpenARK Libraries#include &quot;Core.h&quot;#include &quot;RS2Camera.h&quot;#include &quot;Visualizer.h&quot;using namespace ark;int main() &#123; // change the SR300Camera to other type if needed ark::DepthCamera::Ptr camera = std::make_shared&lt;ark::RS2Camera&gt;(); ark::DetectionParams::Ptr params = ark::DetectionParams::create(); //ark::HandDetector handDetector(true, params); PlaneDetector::Ptr planeDetector = std::make_shared&lt;PlaneDetector&gt;(); HandDetector::Ptr handDetector = std::make_shared&lt;HandDetector&gt;(planeDetector); //ark::HandDetector handDetector(false, params); // change first argument to true to remove planes in frame camera-&gt;beginCapture(); int frame = 0; while (true) &#123; // Show image cv::Mat xyzVisual; ark::Visualizer::visualizeXYZMap(camera-&gt;getXYZMap(), xyzVisual); cv::imshow(&quot;XYZ Map&quot;, xyzVisual); // Find hands handDetector-&gt;update(*camera); auto hands = handDetector-&gt;getHands(); if (!hands.empty()) &#123; ark::Hand::Ptr hand = hands[0]; // Show the hand cv::Mat visual = camera-&gt;getXYZMap().clone(); ark::Visualizer::visualizeHand(visual, visual, hand.get(), hand-&gt;getSVMConfidence()); cv::imshow(&quot;Result&quot;, visual); &#125; /**** Start: Loop Break Condition ****/ int c = cv::waitKey(1); if (c == &apos;q&apos; || c == &apos;Q&apos; || c == 27) &#123; break; &#125; /**** End: Loop Break Condition ****/ ++frame; &#125; return 0;&#125;]]></content>
      <categories>
        <category>handDetect</category>
      </categories>
      <tags>
        <tag>realsense</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[faster-rcnn]]></title>
    <url>%2F2018%2F06%2F15%2Ffaster-rcnn%2F</url>
    <content type="text"><![CDATA[Faster-rcnn 理解 Region Proposal NetworksA Region Proposal Network (RPN) takes an image (of any size) as input and outputs a set of rectangular object proposals, each with an objectness score.3 We model this process with a fully convolutional network [7], which we describe in this section. Because our ultimate goal is to share computation with a Fast R-CNN object detection network [2], we assume that both nets share a common set of convolutional layers. In our experiments, we investigate the Zeiler and Fergus model [32] (ZF), which has 5 shareable convolutional layers and the Simonyan and Zisserman model [3] (VGG-16), which has 13 shareable convolutional layers. To generate region proposals, we slide a small network over the convolutional feature map output by the last shared convolutional layer. This small network takes as input an n × n spatial window of the input convolutional feature map. Each sliding window is mapped to a lower-dimensional feature (256-d for ZF and 512-d for VGG, with ReLU [33] following). This feature is fed into two sibling fullyconnected layers—a box-regression layer (reg) and a box-classification layer (cls). We use n = 3 in this paper, noting that the effective receptive field on the input image is large (171 and 228 pixels for ZF and VGG, respectively). This mini-network is illustrated at a single position in Figure 3 (left). Note that because the mini-network operates in a sliding-window fashion, the fully-connected layers are shared across all spatial locations. This architecture is naturally implemented with an n×n convolutional layer followed by two sibling 1 × 1 convolutional layers (for reg and cls, respectively). The region proposal layer consists of a Region Proposal Network and three layers – Proposal Layer, Anchor Target Layer and Proposal Target Layer.]]></content>
      <categories>
        <category>faster-rcnn</category>
      </categories>
      <tags>
        <tag>faster-rcnn object-detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[install_tmux]]></title>
    <url>%2F2018%2F06%2F11%2Finstall-tmux%2F</url>
    <content type="text"><![CDATA[如何在没有root权限的情况下安装tmux 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#!/bin/bash# tmux will be installed in $INSTALL_DIR/local/bin.# It's assumed that wget and a C/C++ compiler are installed.# exit on errorset -eTMUX_VERSION=2.7INSTALL_DIR=/home/dongzx13/tools# create our directoriesmkdir -p $INSTALL_DIR/local $INSTALL_DIR/tmux_tmpcp ncurses-5.9.tar.gz $INSTALL_DIR/tmux_tmpcp libevent-2.1.8-stable.tar.gz $INSTALL_DIR/tmux_tmpcp tmux-2.7.tar.gz $INSTALL_DIR/tmux_tmpcd $INSTALL_DIR/tmux_tmp# download source files for tmux, libevent, and ncurses# extract files, configure, and compile############# libevent #############tar xvzf libevent-2.1.8-stable.tar.gzcd libevent-2.1.8-stable./configure --prefix=$INSTALL_DIR/local --disable-sharedmakemake installcd ..############# ncurses #############if [[ $(fs --version) =~ "afs" ]] &amp;&amp; fs whereis "$HOME/local" ; then NCURSES_OPTION=" --enable-symlinks"else NCURSES_OPTION=""fitar xvzf ncurses-5.9.tar.gzcd ncurses-5.9./configure --prefix=$INSTALL_DIR/local $NCURSES_OPTIONmakemake installcd ..############# tmux #############tar xvzf tmux-$&#123;TMUX_VERSION&#125;.tar.gzcd tmux-$&#123;TMUX_VERSION&#125;sh ./autogen.sh./configure CFLAGS="-I$INSTALL_DIR/local/include -I$INSTALL_DIR/local/include/ncurses" LDFLAGS="-L$INSTALL_DIR/local/lib -L$INSTALL_DIR/local/include/ncurses -L$INSTALL_DIR/local/include"CPPFLAGS="-I$INSTALL_DIR/local/include -I$INSTALL_DIR/local/include/ncurses" LDFLAGS="-static -L$INSTALL_DIR/local/include -L$INSTALL_DIR/local/include/ncurses -L$INSTALL_DIR/local/lib" makecp tmux $INSTALL_DIR/local/bincd ..# cleanuprm -rf $INSTALL_DIR/tmux_tmpecho "$INSTALL_DIR/local/bin/tmux is now available. You can optionally add $INSTALL_DIR/local/bin to your PATH." 参考: no root to install tmux]]></content>
      <tags>
        <tag>install_software_no_root</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ctpn]]></title>
    <url>%2F2018%2F06%2F09%2Fctpn%2F</url>
    <content type="text"><![CDATA[准备数据集 将数据集转化为pascal voc的recognition格式 代码: prepare_img Annotations 标注 0368.xml … ImageSets Layout Main trainval.txt这个文件中的放的是图像文件的名字去掉&lt;.jpg&gt; Segmentation JPEGImages 图像文件 0368.jpg … SegmentationClass 用来分割 SegmentationObject 用来分割 如何使用这个数据集 代码: datasets ds_utils.py 代码解读: factory.py 工厂类 1lambda split=split, year=year: pascal_voc(split, year) 解读: lambda 函数 imdb.py assert语句用来声明某个条件是真的 pascal_voc.py]]></content>
      <categories>
        <category>text-detection</category>
      </categories>
      <tags>
        <tag>text-detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AdaBoost]]></title>
    <url>%2F2018%2F06%2F03%2Fadaboost%2F</url>
    <content type="text"><![CDATA[AdaBoost, is short for adaptive boosting. Boosting can give good results even if the base classifiers have a performance that is only slightly better than random, and hence sometimes the base classifiers are known as weak learners. Originally designed for solving classification problems, boosting can also be extended to regression. Adaboost算法流程 初始化每一个数据的权重系数 weighting coefficients {$w_n$}: $w_n^{(1)}$ = 1/N for n = 1,…,N. For m = 1,…,M: 在训练数据上训练 一个分类器classifier $y_m(x)$ ,使得加权的错误率最低:$$J_m = \sum_{n=1}^Nw_n^{(m)}I(y_m( {\textbf x_n})\neq t_n)$$ ​ 其中$I(y_m({\textbf x_n})\neq t_n)$是指示函数,当$y_m({\textbf x_n})\neq t_n$时,这个函数为1,其余的时候为0. 计算下面的式子:$$\epsilon_{m}=\frac{\sum_{n=1}^{N}w_{n}^{(m)}I(y_{m}(\textbf x_n)\ne t_{n})}{\sum_{n=1}^{N}w_{n}^{(m)}}$$利用上面的式子去计算$\alpha_m​$:$$\alpha_{m}=ln\{\frac{1-\epsilon_{m}}{\epsilon_{m}}\}$$ 更新权重系数$$w_{n}^{(m+1)}=w_{n}^{(m)}exp[ \alpha_{m}I(y_{m}(\textbf x_n)\neq t_{n}) ]$$ 利用最后的模型做出预测:$$Y_M(\textbf {x}) = sign(\sum_{m=1}^M\alpha_my_m(\textbf x))$$ 理论解释—对指数误差函数的顺序最小化指数误差函数:$$E=\sum_{n=1}^{N}exp[ -t_{n}f_{m}(\textbf x_n) ]$$ 其中$f_{m}(\textbf x)$可以看作是基础分类器$y_{l}(\textbf x)$的线性组合:$$f_{m}(\textbf x)=\frac{1}{2}\sum_{l=1}^{m}\alpha_{l}y_{l}(\textbf x)$$ ​ 且$t_n\in\{ -1,1\}$是训练的目标值.我们的目标是根据权重系数$\alpha_{l}$ 以及$y_l(\textbf x)$ 来最小化指数误差函数E. 在这里我们假定基础的分类器$y_{1}(\textbf{x}), …, y_{m-1}(\textbf{x})$ 以及他们的系数$\alpha_{1}, …, \alpha_{m-1}$是固定的,我们不对误差函数做全局的最小化,而是仅仅针对$\alpha_{m}$ and $y_{m}(\textbf{x})$做最小化. 因此我们重写这个误差函数:$$E =\sum_{n=1}^{N}exp\{-t_{n}f_{m-1}(\textbf x_n)-\frac{1}{2}t_{n}\alpha_{m}y_{m}(\textbf x_n)\} \=\sum_{n=1}^{N}w_{n}^{(m)}exp\{-\frac{1}{2}t_{n}\alpha_{m}y_{m}(\textbf x_n)\}$$其中在我们仅对$\alpha_{m}$ and $y_{m}(\textbf{x})$ 做最优化, 所以$w_{n}^{(m)}=exp\{-t_{n}f_{m-1}(\textbf x_n)\}$可以被认为是常数. 如果我们用$\Gamma_{m}$代表被 $y_{m}(\textbf{x})$ 分对的样本点, $\mathcal M_m$ 代表分错的样本点,那么我们可以把错误方差能写成如下的形式$$\begin{align}E &amp; = e^{-\alpha_{m}/2}\sum_{n\in \Gamma_{m}}w_{n}^{(m)}+e^{\alpha_{m}/2}\sum_{n\in \mathcal M_m}w_{n}^{(m)} \\&amp; = e^{-\alpha_m/2}\sum_{n\in \Gamma_{m}}w_{n}^{(m)}+e^{-\alpha_{m}/2}\sum_{n\in \mathcal M_m}w_{n}^{(m)}+(e^{\alpha_{m}/2}-e^{-\alpha_{m}/2})\sum_{n\in \mathcal M_m}w_{n}^{(m)} \\&amp; = e^{-\alpha_{m}/2}\sum_{n=1}^Nw_{n}^{(m)}+(e^{\alpha_{m}/2}-e^{-\alpha_{m}/2})\sum_{n=1}^Nw_{n}^{(m)}I(y_{m}(\textbf x_n)\neq t_{n})\end{align}$$当我们对$y_{m}(\textbf{x})$求导的时候,第一项是常数,所以对错误方差求最小值就相当于在adaboost算法中最小化$J_m$ ,因为前面的系数不会影响最小值的位置.相似的,我们对$\alpha_{m}$求导,$$\frac{\partial E}{\partial \alpha_{m}} = -\frac{1}{2}e^{-\alpha_{m}/2}\sum_{n=1}^Nw_{n}^{(m)}+\frac{1}{2}(e^{\alpha_{m}/2}+e^{-\alpha_{m}/2})\sum_{n=1}^Nw_{n}^{(m)}I(y_{m}(\textbf x_n))\neq t_{n})=0$$ $$ \epsilon_{m}=\frac{\sum_{n=1}^{N}w_{n}^{(m)}I(y_{m}(\textbf x_n)\neq t_{n})}{\sum_{n=1}^{N}w_{n}^{(m)}} = \frac{e^{-\alpha_{m}/2}}{e^{\alpha_{m}/2}+e^{-\alpha_{m}/2}}$$ $$\frac{1}{\epsilon_{m}}= \frac{e^{\alpha_{m}/2}+e^{-\alpha_{m}/2}}{e^{-\alpha_{m}/2}}=1+e^{\alpha_{m}}$$ $$\alpha_{m}=ln(\frac{1}{\epsilon_{m}}-1)=ln((1-\epsilon_{m})/\epsilon_{m})$$ 与adaboost算法中一致. 由于$w_{n}^{(m)}=exp\{-t_{n}f_{m-1}(\textbf x_n)\}$,所以$$w_{n}^{(m+1)}=w_{n}^{(m)}exp\{-\frac{1}{2}t_{n}\alpha_{m}y_{m}(\textbf x_n)\}$$由于$$t_{n}y_{m}(\textbf x_n) = 1 - 2I(y_{m}(\textbf x_n)\neq t_{n})$$所以$$\begin{align} w_{n}^{(m+1)}&amp;=w_{n}^{(m)}exp\{-\frac{1}{2}t_{n}\alpha_{m}y_{m}(\textbf x_n)\} \\ &amp;= w_{n}^{(m)}exp\{-\frac{1}{2}\alpha_{m}(1 - 2I(y_{m}(\text x_n)\neq t_{n}) \\ &amp;=w_{n}^{(m)}exp\{-\frac{1}{2}\alpha_{m}\}exp\{\alpha_{m}I(y_{m}(\textbf x_n)\neq t_{n})\}\end{align}$$因为$exp\{-\frac{1}{2}\alpha_{m}\}$与n无关,每次更新的时候都是个常数,所以可以删除.$$w_{n}^{(m+1)}=w_{n}^{(m)}exp\{\alpha_{m}I(y_{m}(\textbf x_n)\neq t_{n})\}$$根据$f_{m}(\textbf x)=\frac{1}{2}\sum_{l=1}^{m}\alpha_{l}y_{l}(\textbf x)$的符号,我们可以得到与adaboost一致的结果,系数$\frac{1}{2}$可以舍掉. 基础Adaboost算法实践 data $X \in \mathbb{R}^{n\times p}$ label $y\in\{-1, +1\}^{n}$ weak learner 是 decision stump. $a\in \mathbb{R}$, $j\in \{1, …, p\}$, $d\in \{-1, +1\}$. 其中 $\textbf{x}\in \mathbb{R}^{p}$ 是个向量,$x_{j}$ 是第$j$个元素. 数据地址:ada_data.mat 我们有1000个训练数据,每一个训练数据有25个特征,以及相应的一个label. 首先初始化weight,$\{\textbf x_i, y_i, w_i\}_{i=1}^n, w_i \geq 0,\sum_{i=1}^nw_i=1$ 然后我们在 decision_stump.m 中返回decision stump的参数,这个参数是最小化加权的错误率$l(a^{\star}, d^{\star}, j^{\star})=min_{a, d, j}l(a, d, j)=min_{a, d, j}\sum_{i=1}^{n}w_{i}1\{h_{a, d, j}(\textbf x_i)\neq y_{i}\}$. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354function [k, a, d] = decision_stump(X, y, w)% decision_stump returns a rule ...% h(x) = d if x(k) ‚, ‚ otherwise,%% Input% X : n * p matrix, each row a sample% y : n * 1 vector, each row a label% w : n * 1 vector, each row a weight%% Output% k : the optimal dimension% a : the optimal threshold% d : the optimal d, 1 or -1% total time complexity required to be O(p*n*logn) or less%%% Your Code Here %%%[m,n] = size(X);minerror = inf;minerror_a = 0;minerror_j = -1;minerror_d = 0;w = w';for i = 1:n comp = repmat(X(:,i),1,m)'; ori_rep = repmat(X(:,i),1,m); result_gt = double(ones(m,m)); result_gt((ori_rep - comp)&gt;=0) = -1.0; label_gt = repmat(y,1,m); [min_value_gt,argmin_gt] = min(w*(result_gt ~= label_gt)); result_lt = double(ones(m,m)); result_lt((ori_rep - comp)&lt;=0) = -1.0; label_lt = repmat(y,1,m); [min_value_lt,argmin_lt] = min(w*(result_lt ~= label_lt)); if min_value_lt &lt; min_value_gt final_error = min_value_lt; final_a = X(argmin_lt,i); final_d = -1.0; else final_error = min_value_gt; final_a = X(argmin_gt,i); final_d = 1.0; end if minerror &gt; final_error minerror = final_error minerror_a = final_a; minerror_d = final_d; minerror_j = i; endendk = minerror_j;a = minerror_a;d = minerror_d;end decision_stump_error.m update_weights.m adaboost_error.m adaboost.m 1234567891011121314151617181920212223242526function w_update = update_weights(X, y, k, a, d, w, alpha)% update_weights update the weights with the recent classifier% % Input% X : n * p matrix, each row a sample% y : n * 1 vector, each row a label% k : selected dimension of features% a : selected threshold for feature-k% d : 1 or -1% w : n * 1 vector, old weights% alpha : weights of the classifiers%% Output% w_update : n * 1 vector, the updated weights%%% Your Code Here %%%p = ((X(:, k) &lt;= a) - 0.5) * 2 * d;result = p;w_update = w.*exp(alpha * (result ~= y));%% 注意要normalization 保证公式分母是一样的w_update = w_update/sum(w_update);%%% Your code Here %%%end ​]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Boost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo-note]]></title>
    <url>%2F2018%2F06%2F01%2Fhexo-note%2F</url>
    <content type="text"><![CDATA[mathjax 不能可以使用bold利用textbf代替 mathjax 不能转译\{,\}导致没有办法输出大括号 编辑node_modules\marked\lib\marked.js 脚本， 将464 escape: /^\(\`*{}[\# +-.!_&gt;])/, 替换为 escape: /^\(\`*[\# +-.!_&gt;])/, 这一步取消了对\{,\}的转义(escape)]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
