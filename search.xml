<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Const_C++]]></title>
    <url>%2F2018%2F07%2F30%2FConst-C%2F</url>
    <content type="text"><![CDATA[Const c++的使用总结 12345const std::vector&lt;Hand::Ptr&gt; &amp; HandDetector::getHands() const &#123; return hands; &#125;//const函数重载 基本知识 const修饰常量时，定义时必须初始化 对于类中的const成员必须通过初始化列表进行初始化 123456789101112131415class A&#123; public: A(int i); void print(); const int &amp;r; private: const int a; static const int b;&#125;;const int A::b=10;A::A(int i):a(i),r(a)&#123; &#125; Const 默认在文件中为局部变量，如果需要在别的文件中访问需要显示的定义为外部变量，非const的变量默认为外部变量 很有趣的地方： 1234567891011#include &lt;iostream&gt;int main(int argc, const char * argv[]) &#123; // insert code here... double a = 3.14; const int &amp;ri = a; // Non-const lvalue reference to type 'int' cannot bind to a value of unrelated type 'double' int &amp;b = a; std::cout &lt;&lt; "Hello, World!\n"; return 0;&#125; 非const引用只能绑定到与该引用相同类型的对象。 const引用则可以绑定到不同但相关的类型的对象或绑定到右值。 原因在于编译器在编译的时候，对于引用存放的是一个对象的地址，对于不可寻址的值，如文字常量以及不同类型的对象，编译器为了实现引用，必须生成一个临时对象，引用实际上指向该对象，但用户不能访问它。ri是const所以没有办法修改的ri的赋值，但是对于b一旦修改了b的值我们希望同时修改a的值，但是由于编译器其实修改的是临时对象temp的值，而不是a的值，所以会造成误解，所以在编译的时候回报错。 const对象的动态数组 1234const int *temp = new const int[100]();//const的修饰的数组必须初始化，所以使用这种方式可以zero initial整个数组const std::string *temp = new std::string[2];//会调用string类的默认构造函数初始化//c++允许定义类类型的const数组，但是必须提供默认构造函数 指针和const修饰符 指向const的指针123456789101112const double *cptr;// cptr是一个指向double类型的const对象的指针，cptr可以指向不同的const对象，但是不能通过cptr修改对象的值。*ctpn = 42;//错误，不能修改值const double pi = 3.14；double *ptr = $pi; //错误，不能把一个const的对象的地址赋给一个普通的非const指针，因为这样可以修改const的值const double *ctpr = &amp;pi;//可以的//允许把非const对象的值赋给const对象的指针 但是不能用const对象的指针来修改其值，修改方法需要借助别的非const指针double dval = 3.13;cptr = &amp;dval;double *ptr = &amp;dval;*ptr =3.14;cout&lt;&lt; *cptr;//结果是3.14 自以为指向const的指针，偷偷修改它也不知道 常指针 本身的值不能修改，与其他const量一样，需要在定义的时候初始化。 12int a = 0;int *const ptr = &amp;a;//能不能修改指向对象的值，取决于指向对象本身的类型 函数和const限定符的关系 类中的const成员函数 在一个类中，任何不会修改数据成员的函数都应该声明为const类型。如果在编写const成员函数时，不慎修改了数据成员，或者调用了其它非const成员函数，编译器将指出错误，这无疑会提高程序的健壮性。使用const关键字进行说明的成员函数，称为常成员函数。只有常成员函数才有资格操作常量或常对象，没有使用const关键字说明的成员函数不能用来操作常对象。常成员函数说明格式如下： &lt;类型说明符&gt; &lt;函数名&gt; (&lt;参数表&gt;) const； 12345678910111213141516class tempstack&#123;private: int m_num; int m_data[100];public: void Push(int elem); int Pop(void); int GetCount(void) const;&#125;;int tempstack::GetCount(void) const&#123; //++m_num;//编译错误Cannot assign to non-static data member within const member function 'GetCount' //Pop();//Member function 'Pop' not viable: 'this' argument has type 'const tempstack', but function is not marked const return m_num;&#125; 函数重载 123456789101112131415161718192021222324252627282930#include &lt;iostream&gt;class R&#123;public: R(int r1,int r2) &#123; R1 = r1; R2 = r2; &#125; void print(); void print() const;private: int R1,R2;&#125;;void R::print()&#123; std::cout&lt;&lt;R1;&#125;void R::print() const&#123; std::cout&lt;&lt;R2;&#125;int main(int argc, const char * argv[]) &#123; R a(1,2); a.print();//1 const R b(3,4); b.print();//4 return 0;&#125; const 对象默认调用const成员函数]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[handDetect]]></title>
    <url>%2F2018%2F07%2F27%2FhandDetect%2F</url>
    <content type="text"><![CDATA[使用RealSense进行手势识别 原理 Compute a depth map The depth map is constructed by analyzing a speckle pattern of infrared laser light 具体建立结构光的原理是受专利保护的 Structured light general principle: project a known pattern onto the scene and infer depth from the deformation of that pattern The Kinect combines structured light with two classic computer vision techniques: depth from focus, and depth from stereo Depth from focus uses the principle that stuff that is more blurry is further away 越远的物体就会更模糊 The astigmatic lens causes a projected circle to become an ellipse whose orientation depends on depth 像散透镜使投影圆变成椭圆，其取向取决于深度 Depth from stereo uses parallax 2.infer body postion]]></content>
      <categories>
        <category>handDetect</category>
      </categories>
      <tags>
        <tag>realsense</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[faster-rcnn]]></title>
    <url>%2F2018%2F06%2F15%2Ffaster-rcnn%2F</url>
    <content type="text"><![CDATA[Faster-rcnn 理解 Region Proposal NetworksA Region Proposal Network (RPN) takes an image (of any size) as input and outputs a set of rectangular object proposals, each with an objectness score.3 We model this process with a fully convolutional network [7], which we describe in this section. Because our ultimate goal is to share computation with a Fast R-CNN object detection network [2], we assume that both nets share a common set of convolutional layers. In our experiments, we investigate the Zeiler and Fergus model [32] (ZF), which has 5 shareable convolutional layers and the Simonyan and Zisserman model [3] (VGG-16), which has 13 shareable convolutional layers. To generate region proposals, we slide a small network over the convolutional feature map output by the last shared convolutional layer. This small network takes as input an n × n spatial window of the input convolutional feature map. Each sliding window is mapped to a lower-dimensional feature (256-d for ZF and 512-d for VGG, with ReLU [33] following). This feature is fed into two sibling fullyconnected layers—a box-regression layer (reg) and a box-classification layer (cls). We use n = 3 in this paper, noting that the effective receptive field on the input image is large (171 and 228 pixels for ZF and VGG, respectively). This mini-network is illustrated at a single position in Figure 3 (left). Note that because the mini-network operates in a sliding-window fashion, the fully-connected layers are shared across all spatial locations. This architecture is naturally implemented with an n×n convolutional layer followed by two sibling 1 × 1 convolutional layers (for reg and cls, respectively). The region proposal layer consists of a Region Proposal Network and three layers – Proposal Layer, Anchor Target Layer and Proposal Target Layer.]]></content>
      <categories>
        <category>faster-rcnn</category>
      </categories>
      <tags>
        <tag>faster-rcnn object-detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[install_tmux]]></title>
    <url>%2F2018%2F06%2F11%2Finstall-tmux%2F</url>
    <content type="text"><![CDATA[如何在没有root权限的情况下安装tmux 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#!/bin/bash# tmux will be installed in $INSTALL_DIR/local/bin.# It's assumed that wget and a C/C++ compiler are installed.# exit on errorset -eTMUX_VERSION=2.7INSTALL_DIR=/home/dongzx13/tools# create our directoriesmkdir -p $INSTALL_DIR/local $INSTALL_DIR/tmux_tmpcp ncurses-5.9.tar.gz $INSTALL_DIR/tmux_tmpcp libevent-2.1.8-stable.tar.gz $INSTALL_DIR/tmux_tmpcp tmux-2.7.tar.gz $INSTALL_DIR/tmux_tmpcd $INSTALL_DIR/tmux_tmp# download source files for tmux, libevent, and ncurses# extract files, configure, and compile############# libevent #############tar xvzf libevent-2.1.8-stable.tar.gzcd libevent-2.1.8-stable./configure --prefix=$INSTALL_DIR/local --disable-sharedmakemake installcd ..############# ncurses #############if [[ $(fs --version) =~ "afs" ]] &amp;&amp; fs whereis "$HOME/local" ; then NCURSES_OPTION=" --enable-symlinks"else NCURSES_OPTION=""fitar xvzf ncurses-5.9.tar.gzcd ncurses-5.9./configure --prefix=$INSTALL_DIR/local $NCURSES_OPTIONmakemake installcd ..############# tmux #############tar xvzf tmux-$&#123;TMUX_VERSION&#125;.tar.gzcd tmux-$&#123;TMUX_VERSION&#125;sh ./autogen.sh./configure CFLAGS="-I$INSTALL_DIR/local/include -I$INSTALL_DIR/local/include/ncurses" LDFLAGS="-L$INSTALL_DIR/local/lib -L$INSTALL_DIR/local/include/ncurses -L$INSTALL_DIR/local/include"CPPFLAGS="-I$INSTALL_DIR/local/include -I$INSTALL_DIR/local/include/ncurses" LDFLAGS="-static -L$INSTALL_DIR/local/include -L$INSTALL_DIR/local/include/ncurses -L$INSTALL_DIR/local/lib" makecp tmux $INSTALL_DIR/local/bincd ..# cleanuprm -rf $INSTALL_DIR/tmux_tmpecho "$INSTALL_DIR/local/bin/tmux is now available. You can optionally add $INSTALL_DIR/local/bin to your PATH." 参考: no root to install tmux]]></content>
      <tags>
        <tag>install_software_no_root</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ctpn]]></title>
    <url>%2F2018%2F06%2F09%2Fctpn%2F</url>
    <content type="text"><![CDATA[准备数据集 将数据集转化为pascal voc的recognition格式 代码: prepare_img Annotations 标注 0368.xml … ImageSets Layout Main trainval.txt这个文件中的放的是图像文件的名字去掉&lt;.jpg&gt; Segmentation JPEGImages 图像文件 0368.jpg … SegmentationClass 用来分割 SegmentationObject 用来分割 如何使用这个数据集 代码: datasets ds_utils.py 代码解读: factory.py 工厂类 1lambda split=split, year=year: pascal_voc(split, year) 解读: lambda 函数 imdb.py assert语句用来声明某个条件是真的 pascal_voc.py]]></content>
      <categories>
        <category>text-detection</category>
      </categories>
      <tags>
        <tag>text-detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AdaBoost]]></title>
    <url>%2F2018%2F06%2F02%2Fadaboost%2F</url>
    <content type="text"><![CDATA[AdaBoost, is short for adaptive boosting. Boosting can give good results even if the base classifiers have a performance that is only slightly better than random, and hence sometimes the base classifiers are known as weak learners. Originally designed for solving classification problems, boosting can also be extended to regression. Adaboost算法流程 初始化每一个数据的权重系数 weighting coefficients {$w_n$}: $w_n^{(1)}$ = 1/N for n = 1,…,N. For m = 1,…,M: 在训练数据上训练 一个分类器classifier $y_m(x)$ ,使得加权的错误率最低:$$J_m = \sum_{n=1}^Nw_n^{(m)}I(y_m( {\textbf x_n})\neq t_n)$$ ​ 其中$I(y_m({\textbf x_n})\neq t_n)$是指示函数,当$y_m({\textbf x_n})\neq t_n$时,这个函数为1,其余的时候为0. 计算下面的式子:$$\epsilon_{m}=\frac{\sum_{n=1}^{N}w_{n}^{(m)}I(y_{m}(\textbf x_n)\ne t_{n})}{\sum_{n=1}^{N}w_{n}^{(m)}}$$利用上面的式子去计算$\alpha_m​$:$$\alpha_{m}=ln\{\frac{1-\epsilon_{m}}{\epsilon_{m}}\}$$ 更新权重系数$$w_{n}^{(m+1)}=w_{n}^{(m)}exp[ \alpha_{m}I(y_{m}(\textbf x_n)\neq t_{n}) ]$$ 利用最后的模型做出预测:$$Y_M(\textbf {x}) = sign(\sum_{m=1}^M\alpha_my_m(\textbf x))$$ 理论解释—对指数误差函数的顺序最小化指数误差函数:$$E=\sum_{n=1}^{N}exp[ -t_{n}f_{m}(\textbf x_n) ]$$ 其中$f_{m}(\textbf x)$可以看作是基础分类器$y_{l}(\textbf x)$的线性组合:$$f_{m}(\textbf x)=\frac{1}{2}\sum_{l=1}^{m}\alpha_{l}y_{l}(\textbf x)$$ ​ 且$t_n\in\{ -1,1\}$是训练的目标值.我们的目标是根据权重系数$\alpha_{l}$ 以及$y_l(\textbf x)$ 来最小化指数误差函数E. 在这里我们假定基础的分类器$y_{1}(\textbf{x}), …, y_{m-1}(\textbf{x})$ 以及他们的系数$\alpha_{1}, …, \alpha_{m-1}$是固定的,我们不对误差函数做全局的最小化,而是仅仅针对$\alpha_{m}$ and $y_{m}(\textbf{x})$做最小化. 因此我们重写这个误差函数:$$E =\sum_{n=1}^{N}exp\{-t_{n}f_{m-1}(\textbf x_n)-\frac{1}{2}t_{n}\alpha_{m}y_{m}(\textbf x_n)\} \=\sum_{n=1}^{N}w_{n}^{(m)}exp\{-\frac{1}{2}t_{n}\alpha_{m}y_{m}(\textbf x_n)\}$$其中在我们仅对$\alpha_{m}$ and $y_{m}(\textbf{x})$ 做最优化, 所以$w_{n}^{(m)}=exp\{-t_{n}f_{m-1}(\textbf x_n)\}$可以被认为是常数. 如果我们用$\Gamma_{m}$代表被 $y_{m}(\textbf{x})$ 分对的样本点, $\mathcal M_m$ 代表分错的样本点,那么我们可以把错误方差能写成如下的形式$$\begin{align}E &amp; = e^{-\alpha_{m}/2}\sum_{n\in \Gamma_{m}}w_{n}^{(m)}+e^{\alpha_{m}/2}\sum_{n\in \mathcal M_m}w_{n}^{(m)} \\&amp; = e^{-\alpha_m/2}\sum_{n\in \Gamma_{m}}w_{n}^{(m)}+e^{-\alpha_{m}/2}\sum_{n\in \mathcal M_m}w_{n}^{(m)}+(e^{\alpha_{m}/2}-e^{-\alpha_{m}/2})\sum_{n\in \mathcal M_m}w_{n}^{(m)} \\&amp; = e^{-\alpha_{m}/2}\sum_{n=1}^Nw_{n}^{(m)}+(e^{\alpha_{m}/2}-e^{-\alpha_{m}/2})\sum_{n=1}^Nw_{n}^{(m)}I(y_{m}(\textbf x_n)\neq t_{n})\end{align}$$当我们对$y_{m}(\textbf{x})$求导的时候,第一项是常数,所以对错误方差求最小值就相当于在adaboost算法中最小化$J_m$ ,因为前面的系数不会影响最小值的位置.相似的,我们对$\alpha_{m}$求导,$$\frac{\partial E}{\partial \alpha_{m}} = -\frac{1}{2}e^{-\alpha_{m}/2}\sum_{n=1}^Nw_{n}^{(m)}+\frac{1}{2}(e^{\alpha_{m}/2}+e^{-\alpha_{m}/2})\sum_{n=1}^Nw_{n}^{(m)}I(y_{m}(\textbf x_n))\neq t_{n})=0$$ $$ \epsilon_{m}=\frac{\sum_{n=1}^{N}w_{n}^{(m)}I(y_{m}(\textbf x_n)\neq t_{n})}{\sum_{n=1}^{N}w_{n}^{(m)}} = \frac{e^{-\alpha_{m}/2}}{e^{\alpha_{m}/2}+e^{-\alpha_{m}/2}}$$ $$\frac{1}{\epsilon_{m}}= \frac{e^{\alpha_{m}/2}+e^{-\alpha_{m}/2}}{e^{-\alpha_{m}/2}}=1+e^{\alpha_{m}}$$ $$\alpha_{m}=ln(\frac{1}{\epsilon_{m}}-1)=ln((1-\epsilon_{m})/\epsilon_{m})$$ 与adaboost算法中一致. 由于$w_{n}^{(m)}=exp\{-t_{n}f_{m-1}(\textbf x_n)\}$,所以$$w_{n}^{(m+1)}=w_{n}^{(m)}exp\{-\frac{1}{2}t_{n}\alpha_{m}y_{m}(\textbf x_n)\}$$由于$$t_{n}y_{m}(\textbf x_n) = 1 - 2I(y_{m}(\textbf x_n)\neq t_{n})$$所以$$\begin{align} w_{n}^{(m+1)}&amp;=w_{n}^{(m)}exp\{-\frac{1}{2}t_{n}\alpha_{m}y_{m}(\textbf x_n)\} \\ &amp;= w_{n}^{(m)}exp\{-\frac{1}{2}\alpha_{m}(1 - 2I(y_{m}(\text x_n)\neq t_{n}) \\ &amp;=w_{n}^{(m)}exp\{-\frac{1}{2}\alpha_{m}\}exp\{\alpha_{m}I(y_{m}(\textbf x_n)\neq t_{n})\}\end{align}$$因为$exp\{-\frac{1}{2}\alpha_{m}\}$与n无关,每次更新的时候都是个常数,所以可以删除.$$w_{n}^{(m+1)}=w_{n}^{(m)}exp\{\alpha_{m}I(y_{m}(\textbf x_n)\neq t_{n})\}$$根据$f_{m}(\textbf x)=\frac{1}{2}\sum_{l=1}^{m}\alpha_{l}y_{l}(\textbf x)$的符号,我们可以得到与adaboost一致的结果,系数$\frac{1}{2}$可以舍掉. 基础Adaboost算法实践 data $X \in \mathbb{R}^{n\times p}$ label $y\in\{-1, +1\}^{n}$ weak learner 是 decision stump. $a\in \mathbb{R}$, $j\in \{1, …, p\}$, $d\in \{-1, +1\}$. 其中 $\textbf{x}\in \mathbb{R}^{p}$ 是个向量,$x_{j}$ 是第$j$个元素. 数据地址:ada_data.mat 我们有1000个训练数据,每一个训练数据有25个特征,以及相应的一个label. 首先初始化weight,$\{\textbf x_i, y_i, w_i\}_{i=1}^n, w_i \geq 0,\sum_{i=1}^nw_i=1$ 然后我们在 decision_stump.m 中返回decision stump的参数,这个参数是最小化加权的错误率$l(a^{\star}, d^{\star}, j^{\star})=min_{a, d, j}l(a, d, j)=min_{a, d, j}\sum_{i=1}^{n}w_{i}1\{h_{a, d, j}(\textbf x_i)\neq y_{i}\}$. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354function [k, a, d] = decision_stump(X, y, w)% decision_stump returns a rule ...% h(x) = d if x(k) ‚, ‚ otherwise,%% Input% X : n * p matrix, each row a sample% y : n * 1 vector, each row a label% w : n * 1 vector, each row a weight%% Output% k : the optimal dimension% a : the optimal threshold% d : the optimal d, 1 or -1% total time complexity required to be O(p*n*logn) or less%%% Your Code Here %%%[m,n] = size(X);minerror = inf;minerror_a = 0;minerror_j = -1;minerror_d = 0;w = w';for i = 1:n comp = repmat(X(:,i),1,m)'; ori_rep = repmat(X(:,i),1,m); result_gt = double(ones(m,m)); result_gt((ori_rep - comp)&gt;=0) = -1.0; label_gt = repmat(y,1,m); [min_value_gt,argmin_gt] = min(w*(result_gt ~= label_gt)); result_lt = double(ones(m,m)); result_lt((ori_rep - comp)&lt;=0) = -1.0; label_lt = repmat(y,1,m); [min_value_lt,argmin_lt] = min(w*(result_lt ~= label_lt)); if min_value_lt &lt; min_value_gt final_error = min_value_lt; final_a = X(argmin_lt,i); final_d = -1.0; else final_error = min_value_gt; final_a = X(argmin_gt,i); final_d = 1.0; end if minerror &gt; final_error minerror = final_error minerror_a = final_a; minerror_d = final_d; minerror_j = i; endendk = minerror_j;a = minerror_a;d = minerror_d;end decision_stump_error.m update_weights.m adaboost_error.m adaboost.m 1234567891011121314151617181920212223242526function w_update = update_weights(X, y, k, a, d, w, alpha)% update_weights update the weights with the recent classifier% % Input% X : n * p matrix, each row a sample% y : n * 1 vector, each row a label% k : selected dimension of features% a : selected threshold for feature-k% d : 1 or -1% w : n * 1 vector, old weights% alpha : weights of the classifiers%% Output% w_update : n * 1 vector, the updated weights%%% Your Code Here %%%p = ((X(:, k) &lt;= a) - 0.5) * 2 * d;result = p;w_update = w.*exp(alpha * (result ~= y));%% 注意要normalization 保证公式分母是一样的w_update = w_update/sum(w_update);%%% Your code Here %%%end ​]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Boost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo-note]]></title>
    <url>%2F2018%2F06%2F01%2Fhexo-note%2F</url>
    <content type="text"><![CDATA[mathjax 不能可以使用bold利用textbf代替 mathjax 不能转译\{,\}导致没有办法输出大括号 编辑node_modules\marked\lib\marked.js 脚本， 将464 escape: /^\(\`*{}[\# +-.!_&gt;])/, 替换为 escape: /^\(\`*[\# +-.!_&gt;])/, 这一步取消了对\{,\}的转义(escape)]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
