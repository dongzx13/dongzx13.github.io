<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[install_tmux]]></title>
    <url>%2F2018%2F06%2F11%2Finstall-tmux%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#!/bin/bash# tmux will be installed in $INSTALL_DIR/local/bin.# It's assumed that wget and a C/C++ compiler are installed.# exit on errorset -eTMUX_VERSION=2.7INSTALL_DIR=/home/dongzx13/tools# create our directoriesmkdir -p $INSTALL_DIR/local $INSTALL_DIR/tmux_tmpcp ncurses-5.9.tar.gz $INSTALL_DIR/tmux_tmpcp libevent-2.1.8-stable.tar.gz $INSTALL_DIR/tmux_tmpcp tmux-2.7.tar.gz $INSTALL_DIR/tmux_tmpcd $INSTALL_DIR/tmux_tmp# download source files for tmux, libevent, and ncurses# extract files, configure, and compile############# libevent #############tar xvzf libevent-2.1.8-stable.tar.gzcd libevent-2.1.8-stable./configure --prefix=$INSTALL_DIR/local --disable-sharedmakemake installcd ..############# ncurses #############if [[ $(fs --version) =~ "afs" ]] &amp;&amp; fs whereis "$HOME/local" ; then NCURSES_OPTION=" --enable-symlinks"else NCURSES_OPTION=""fitar xvzf ncurses-5.9.tar.gzcd ncurses-5.9./configure --prefix=$INSTALL_DIR/local $NCURSES_OPTIONmakemake installcd ..############# tmux #############tar xvzf tmux-$&#123;TMUX_VERSION&#125;.tar.gzcd tmux-$&#123;TMUX_VERSION&#125;sh ./autogen.sh./configure CFLAGS="-I$INSTALL_DIR/local/include -I$INSTALL_DIR/local/include/ncurses" LDFLAGS="-L$INSTALL_DIR/local/lib -L$INSTALL_DIR/local/include/ncurses -L$INSTALL_DIR/local/include"CPPFLAGS="-I$INSTALL_DIR/local/include -I$INSTALL_DIR/local/include/ncurses" LDFLAGS="-static -L$INSTALL_DIR/local/include -L$INSTALL_DIR/local/include/ncurses -L$INSTALL_DIR/local/lib" makecp tmux $INSTALL_DIR/local/bincd ..# cleanuprm -rf $INSTALL_DIR/tmux_tmpecho "$INSTALL_DIR/local/bin/tmux is now available. You can optionally add $INSTALL_DIR/local/bin to your PATH." 参考: no root to install tmux]]></content>
      <tags>
        <tag>install_software_no_root</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ctpn]]></title>
    <url>%2F2018%2F06%2F09%2Fctpn%2F</url>
    <content type="text"><![CDATA[#准备数据集 将数据集转化为pascal voc的recognition格式 代码: prepare_img Annotations 标注 0368.xml … ImageSets Layout Main trainval.txt这个文件中的放的是图像文件的名字去掉&lt;.jpg&gt; Segmentation JPEGImages 图像文件 0368.jpg … SegmentationClass 用来分割 SegmentationObject 用来分割 如何使用这个数据集 代码: datasets ds_utils.py 代码解读: factory.py 工厂类 1lambda split=split, year=year: pascal_voc(split, year) 解读: lambda 函数 imdb.py assert语句用来声明某个条件是真的 pascal_voc.py]]></content>
      <categories>
        <category>text-detection</category>
      </categories>
      <tags>
        <tag>text-detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AdaBoost]]></title>
    <url>%2F2018%2F06%2F03%2Fadaboost%2F</url>
    <content type="text"><![CDATA[AdaBoost, is short for adaptive boosting. Boosting can give good results even if the base classifiers have a performance that is only slightly better than random, and hence sometimes the base classifiers are known as weak learners. Originally designed for solving classification problems, boosting can also be extended to regression. Adaboost算法流程 初始化每一个数据的权重系数 weighting coefficients {$w_n$}: $w_n^{(1)}$ = 1/N for n = 1,…,N. For m = 1,…,M: 在训练数据上训练 一个分类器classifier $y_m(x)$ ,使得加权的错误率最低:$$J_m = \sum_{n=1}^Nw_n^{(m)}I(y_m( {\textbf x_n})\neq t_n)$$ ​ 其中$I(y_m({\textbf x_n})\neq t_n)$是指示函数,当$y_m({\textbf x_n})\neq t_n$时,这个函数为1,其余的时候为0. 计算下面的式子:$$\epsilon_{m}=\frac{\sum_{n=1}^{N}w_{n}^{(m)}I(y_{m}(\textbf x_n)\ne t_{n})}{\sum_{n=1}^{N}w_{n}^{(m)}}$$利用上面的式子去计算$\alpha_m​$:$$\alpha_{m}=ln\{\frac{1-\epsilon_{m}}{\epsilon_{m}}\}$$ 更新权重系数$$w_{n}^{(m+1)}=w_{n}^{(m)}exp[ \alpha_{m}I(y_{m}(\textbf x_n)\neq t_{n}) ]$$ 利用最后的模型做出预测:$$Y_M(\textbf {x}) = sign(\sum_{m=1}^M\alpha_my_m(\textbf x))$$ 理论解释—对指数误差函数的顺序最小化指数误差函数:$$E=\sum_{n=1}^{N}exp[ -t_{n}f_{m}(\textbf x_n) ]$$ 其中$f_{m}(\textbf x)$可以看作是基础分类器$y_{l}(\textbf x)$的线性组合:$$f_{m}(\textbf x)=\frac{1}{2}\sum_{l=1}^{m}\alpha_{l}y_{l}(\textbf x)$$ ​ 且$t_n\in\{ -1,1\}$是训练的目标值.我们的目标是根据权重系数$\alpha_{l}$ 以及$y_l(\textbf x)$ 来最小化指数误差函数E. 在这里我们假定基础的分类器$y_{1}(\textbf{x}), …, y_{m-1}(\textbf{x})$ 以及他们的系数$\alpha_{1}, …, \alpha_{m-1}$是固定的,我们不对误差函数做全局的最小化,而是仅仅针对$\alpha_{m}$ and $y_{m}(\textbf{x})$做最小化. 因此我们重写这个误差函数:$$E =\sum_{n=1}^{N}exp\{-t_{n}f_{m-1}(\textbf x_n)-\frac{1}{2}t_{n}\alpha_{m}y_{m}(\textbf x_n)\} \=\sum_{n=1}^{N}w_{n}^{(m)}exp\{-\frac{1}{2}t_{n}\alpha_{m}y_{m}(\textbf x_n)\}$$其中在我们仅对$\alpha_{m}$ and $y_{m}(\textbf{x})$ 做最优化, 所以$w_{n}^{(m)}=exp\{-t_{n}f_{m-1}(\textbf x_n)\}$可以被认为是常数. 如果我们用$\Gamma_{m}$代表被 $y_{m}(\textbf{x})$ 分对的样本点, $\mathcal M_m$ 代表分错的样本点,那么我们可以把错误方差能写成如下的形式$$\begin{align}E &amp; = e^{-\alpha_{m}/2}\sum_{n\in \Gamma_{m}}w_{n}^{(m)}+e^{\alpha_{m}/2}\sum_{n\in \mathcal M_m}w_{n}^{(m)} \\&amp; = e^{-\alpha_m/2}\sum_{n\in \Gamma_{m}}w_{n}^{(m)}+e^{-\alpha_{m}/2}\sum_{n\in \mathcal M_m}w_{n}^{(m)}+(e^{\alpha_{m}/2}-e^{-\alpha_{m}/2})\sum_{n\in \mathcal M_m}w_{n}^{(m)} \\&amp; = e^{-\alpha_{m}/2}\sum_{n=1}^Nw_{n}^{(m)}+(e^{\alpha_{m}/2}-e^{-\alpha_{m}/2})\sum_{n=1}^Nw_{n}^{(m)}I(y_{m}(\textbf x_n)\neq t_{n})\end{align}$$当我们对$y_{m}(\textbf{x})$求导的时候,第一项是常数,所以对错误方差求最小值就相当于在adaboost算法中最小化$J_m$ ,因为前面的系数不会影响最小值的位置.相似的,我们对$\alpha_{m}$求导,$$\frac{\partial E}{\partial \alpha_{m}} = -\frac{1}{2}e^{-\alpha_{m}/2}\sum_{n=1}^Nw_{n}^{(m)}+\frac{1}{2}(e^{\alpha_{m}/2}+e^{-\alpha_{m}/2})\sum_{n=1}^Nw_{n}^{(m)}I(y_{m}(\textbf x_n))\neq t_{n})=0$$ $$ \epsilon_{m}=\frac{\sum_{n=1}^{N}w_{n}^{(m)}I(y_{m}(\textbf x_n)\neq t_{n})}{\sum_{n=1}^{N}w_{n}^{(m)}} = \frac{e^{-\alpha_{m}/2}}{e^{\alpha_{m}/2}+e^{-\alpha_{m}/2}}$$ $$\frac{1}{\epsilon_{m}}= \frac{e^{\alpha_{m}/2}+e^{-\alpha_{m}/2}}{e^{-\alpha_{m}/2}}=1+e^{\alpha_{m}}$$ $$\alpha_{m}=ln(\frac{1}{\epsilon_{m}}-1)=ln((1-\epsilon_{m})/\epsilon_{m})$$ 与adaboost算法中一致. 由于$w_{n}^{(m)}=exp\{-t_{n}f_{m-1}(\textbf x_n)\}$,所以$$w_{n}^{(m+1)}=w_{n}^{(m)}exp\{-\frac{1}{2}t_{n}\alpha_{m}y_{m}(\textbf x_n)\}$$由于$$t_{n}y_{m}(\textbf x_n) = 1 - 2I(y_{m}(\textbf x_n)\neq t_{n})$$所以$$\begin{align} w_{n}^{(m+1)}&amp;=w_{n}^{(m)}exp\{-\frac{1}{2}t_{n}\alpha_{m}y_{m}(\textbf x_n)\} \\ &amp;= w_{n}^{(m)}exp\{-\frac{1}{2}\alpha_{m}(1 - 2I(y_{m}(\text x_n)\neq t_{n}) \\ &amp;=w_{n}^{(m)}exp\{-\frac{1}{2}\alpha_{m}\}exp\{\alpha_{m}I(y_{m}(\textbf x_n)\neq t_{n})\}\end{align}$$因为$exp\{-\frac{1}{2}\alpha_{m}\}$与n无关,每次更新的时候都是个常数,所以可以删除.$$w_{n}^{(m+1)}=w_{n}^{(m)}exp\{\alpha_{m}I(y_{m}(\textbf x_n)\neq t_{n})\}$$根据$f_{m}(\textbf x)=\frac{1}{2}\sum_{l=1}^{m}\alpha_{l}y_{l}(\textbf x)$的符号,我们可以得到与adaboost一致的结果,系数$\frac{1}{2}$可以舍掉. 基础Adaboost算法实践 data $X \in \mathbb{R}^{n\times p}$ label $y\in\{-1, +1\}^{n}$ weak learner 是 decision stump. $a\in \mathbb{R}$, $j\in \{1, …, p\}$, $d\in \{-1, +1\}$. 其中 $\textbf{x}\in \mathbb{R}^{p}$ 是个向量,$x_{j}$ 是第$j$个元素. 数据地址:ada_data.mat 我们有1000个训练数据,每一个训练数据有25个特征,以及相应的一个label. 首先初始化weight,$\{\textbf x_i, y_i, w_i\}_{i=1}^n, w_i \geq 0,\sum_{i=1}^nw_i=1$ 然后我们在 decision_stump.m 中返回decision stump的参数,这个参数是最小化加权的错误率$l(a^{\star}, d^{\star}, j^{\star})=min_{a, d, j}l(a, d, j)=min_{a, d, j}\sum_{i=1}^{n}w_{i}1\{h_{a, d, j}(\textbf x_i)\neq y_{i}\}$. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354function [k, a, d] = decision_stump(X, y, w)% decision_stump returns a rule ...% h(x) = d if x(k) ‚, ‚ otherwise,%% Input% X : n * p matrix, each row a sample% y : n * 1 vector, each row a label% w : n * 1 vector, each row a weight%% Output% k : the optimal dimension% a : the optimal threshold% d : the optimal d, 1 or -1% total time complexity required to be O(p*n*logn) or less%%% Your Code Here %%%[m,n] = size(X);minerror = inf;minerror_a = 0;minerror_j = -1;minerror_d = 0;w = w';for i = 1:n comp = repmat(X(:,i),1,m)'; ori_rep = repmat(X(:,i),1,m); result_gt = double(ones(m,m)); result_gt((ori_rep - comp)&gt;=0) = -1.0; label_gt = repmat(y,1,m); [min_value_gt,argmin_gt] = min(w*(result_gt ~= label_gt)); result_lt = double(ones(m,m)); result_lt((ori_rep - comp)&lt;=0) = -1.0; label_lt = repmat(y,1,m); [min_value_lt,argmin_lt] = min(w*(result_lt ~= label_lt)); if min_value_lt &lt; min_value_gt final_error = min_value_lt; final_a = X(argmin_lt,i); final_d = -1.0; else final_error = min_value_gt; final_a = X(argmin_gt,i); final_d = 1.0; end if minerror &gt; final_error minerror = final_error minerror_a = final_a; minerror_d = final_d; minerror_j = i; endendk = minerror_j;a = minerror_a;d = minerror_d;end decision_stump_error.m update_weights.m adaboost_error.m adaboost.m 1234567891011121314151617181920212223242526function w_update = update_weights(X, y, k, a, d, w, alpha)% update_weights update the weights with the recent classifier% % Input% X : n * p matrix, each row a sample% y : n * 1 vector, each row a label% k : selected dimension of features% a : selected threshold for feature-k% d : 1 or -1% w : n * 1 vector, old weights% alpha : weights of the classifiers%% Output% w_update : n * 1 vector, the updated weights%%% Your Code Here %%%p = ((X(:, k) &lt;= a) - 0.5) * 2 * d;result = p;w_update = w.*exp(alpha * (result ~= y));%% 注意要normalization 保证公式分母是一样的w_update = w_update/sum(w_update);%%% Your code Here %%%end ​]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Boost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo-note]]></title>
    <url>%2F2018%2F06%2F01%2Fhexo-note%2F</url>
    <content type="text"><![CDATA[mathjax 不能可以使用bold利用textbf代替 mathjax 不能转译\{,\}导致没有办法输出大括号 编辑node_modules\marked\lib\marked.js 脚本， 将464 escape: /^\(\`*{}[\# +-.!_&gt;])/, 替换为 escape: /^\(\`*[\# +-.!_&gt;])/, 这一步取消了对\{,\}的转义(escape)]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
