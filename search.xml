<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Filter Banks and MFCC]]></title>
    <url>%2F2019%2F03%2F15%2FFilter-Banks-and-MFCC%2F</url>
    <content type="text"><![CDATA[Filter Banks and MFCC Filter BanksIn a nutshell, a signal goes through a pre-emphasis filter; then gets sliced into frames and a window function is applied to each frame, afterwards, we do a Fourier transform on each frame(or more specifically a Short-Time Fourier Transform) and caculate the power spectrum, and subsequently compute the filter banks. MFCCTo obtain MFCCs, a Discrete Cosine Transform is applied to the filter banks retaining a number of the resulting coefficients while the rest are discarded. A final step in both cases, is mean normalization.原始语音 pre-emphasisIn order to amplify the high frequencies. balance the frequency spectrum since high frequencies usually have smaller magnitudes compared to lower frequencies Avoid numerical problems during the Fourier transfrom operation improve the signal-to-noise ratio 2是pre-emphasis最modest的effect，1和3可以通过mean normalization 实现。 实现方法： $$y(t) = x(t) - \alpha x(t-1)​$$ which can be easily implemented using the following line, where typical values for the filter coefficient ($\alpha$) are 0.95 or 0.97, pre_emphasis = 0.97 1emphasized_signal = numpy.append(signal[0], signal[1:] - pre_emphasis * signal[:-1]) 经过pre-emphasis的语音 framingBy doing a Fourier transform over this short-time frame, we can obtain a good approximation of the frequency contours of the signal by concatenating adjacent frames. Typical frame sizes in speech processing range from 20ms to 40ms with 50% overlap between consecutive frames. Popular settings are 25ms for the frame size, and a 10ms stride(15ms overlap). windowAfter slicking the signal into frames, we apply a window function such as Hamming window to each frame. A Hamming window has the following form: $w[n] = 0.54 − 0.46 cos ( \frac{2πn}{N − 1} )$ where,$ 0≤n≤N−1$, $N$ is the window length. Plotting the previous equation yields the following plot: There are several reasons why we need to apply a window function to the frames, notably to counteract the ssumption made by the FFT that the data is infinite and to reduce spectral leakage. FFT算法只是对有限长度信号进行变换，有限长度信号相当于无限长信号和矩形窗的乘积，也就是讲这个无限长信号截短，对应频域的傅立叶变换是实际信号傅立叶变换与矩形窗傅立叶变换的卷积，当信号为截距后的频谱不同于它以前的频谱，例如，对于频率为fs的正弦序列，它的频谱应该只有在fs处有离散谱，但是在对它的频谱做了截短后，结果使信号的频谱不只在fs处有离散铺，而是在以fs为中心的频带范围内都有谱线出现，他们可以理解为从fs频率上泄漏出去的，这就是频谱泄漏 1frames *= numpy.hamming(frame_length) fourier-transform and power spectrumWe can now do an N-point FFT on each frame to calculate the frequency spectrum, which is also called Short-Time Fourier-Transform (STFT), where NN is typically 256 or 512, NFFT = 512; and then compute the power spectrum (periodogram) using the following equation: $P = \frac{|FFT(x_i)|^2}{N}$ where, $x_i$ is the $i^{th}$ frame of signal $x$. This could be implemented with the following lines: 12mag_frames = numpy.absolute(numpy.fft.rfft(frames, NFFT)) # Magnitude of the FFTpow_frames = ((1.0 / NFFT) * ((mag_frames) ** 2)) # Power Spectrum filter banksFinal step: applying triangular filters, typically 40 filters, nfilt=40 on a Mel-scale to the power spectrum to extract frequency bands. The Mel-scale aims to mimic the non-linear human ear perception of sound, by being more discriminative at lower frequencies and less discriminative at higher frequencies.We can convert between Hertz (ff) and Mel (mm) using the following equations: $m = 2595 \log_{10} (1 + \frac{f}{700})$ $f = 700 (10^{m/2595} - 1)$ 12345678910111213141516171819low_freq_mel = 0high_freq_mel = (2595 * numpy.log10(1 + (sample_rate / 2) / 700)) # Convert Hz to Melmel_points = numpy.linspace(low_freq_mel, high_freq_mel, nfilt + 2) # Equally spaced in Mel scalehz_points = (700 * (10**(mel_points / 2595) - 1)) # Convert Mel to Hzbin = numpy.floor((NFFT + 1) * hz_points / sample_rate)fbank = numpy.zeros((nfilt, int(numpy.floor(NFFT / 2 + 1))))for m in range(1, nfilt + 1): f_m_minus = int(bin[m - 1]) # left f_m = int(bin[m]) # center f_m_plus = int(bin[m + 1]) # right for k in range(f_m_minus, f_m): fbank[m - 1, k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1]) for k in range(f_m, f_m_plus): fbank[m - 1, k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m])filter_banks = numpy.dot(pow_frames, fbank.T)filter_banks = numpy.where(filter_banks == 0, numpy.finfo(float).eps, filter_banks) # Numerical Stabilityfilter_banks = 20 * numpy.log10(filter_banks) # dB MFCCIt turns out that filter bank coefficients computed in the previous step are highly correlated, which could be problematic in some machine learning algorithms. Therefore, we can apply Discrete Cosine Transform (DCT) to decorrelate the filter bank coefficients and yield a compressed representation of the filter banks. Typically, for Automatic Speech Recognition (ASR), the resulting cepstral coefficients 2-13 are retained and the rest are discarded; num_ceps = 12. The reasons for discarding the other coefficients is that they represent fast changes in the filter bank coefficients and these fine details don’t contribute to Automatic Speech Recognition (ASR). 1mfcc = dct(filter_banks, type=2, axis=1, norm='ortho')[:, 1 : (num_ceps + 1)] # Keep 2-13 One may apply sinusoidal liftering1 to the MFCCs to de-emphasize higher MFCCs which has been claimed to improve speech recognition in noisy signals. 1234(nframes, ncoeff) = mfcc.shapen = numpy.arange(ncoeff)lift = 1 + (cep_lifter / 2) * numpy.sin(numpy.pi * n / cep_lifter)mfcc *= lift #* The resulting MFCCs: MFCCs Mean NormalizationAs previously mentioned, to balance the spectrum and improve the Signal-to-Noise (SNR), we can simply subtract the mean of each coefficient from all frames. 1filter_banks -= (numpy.mean(filter_banks, axis=0) + 1e-8) The mean-normalized filter banks: Normalized Filter Banks and similarly for MFCCs: 1mfcc -= (numpy.mean(mfcc, axis=0) + 1e-8) The mean-normalized MFCCs: Normalized MFCCs Filter Banks vs MFCCsTo this point, the steps to compute filter banks and MFCCs were discussed in terms of their motivations and implementations. It is interesting to note that all steps needed to compute filter banks were motivated by the nature of the speech signal and the human perception of such signals. On the contrary, the extra steps needed to compute MFCCs were motivated by the limitation of some machine learning algorithms. The Discrete Cosine Transform (DCT) was needed to decorrelate filter bank coefficients, a process also referred to as whitening. In particular, MFCCs were very popular when Gaussian Mixture Models - Hidden Markov Models (GMMs-HMMs) were very popular and together, MFCCs and GMMs-HMMs co-evolved to be the standard way of doing Automatic Speech Recognition (ASR)2. With the advent of Deep Learning in speech systems, one might question if MFCCs are still the right choice given that deep neural networks are less susceptible to highly correlated input and therefore the Discrete Cosine Transform (DCT) is no longer a necessary step. It is beneficial to note that Discrete Cosine Transform (DCT) is a linear transformation, and therefore undesirable as it discards some information in speech signals which are highly non-linear. It is sensible to question if the Fourier Transform is a necessary operation. Given that the Fourier Transform itself is also a linear operation, it might be beneficial to ignore it and attempt to learn directly from the signal in the time domain. Indeed, some recent work has already attempted this and positive results were reported. However, the Fourier transform operation is a difficult operation to learn and may arguably increase the amount of data and model complexity needed to achieve the same performance. Moreover, in doing Short-Time Fourier Transform (STFT), we’ve assumed the signal to be stationary within this short time and therefore the linearity of the Fourier transform would not pose a critical problem. ConclusionIn this post, we’ve explored the procedure to compute Mel-scaled filter banks and Mel-Frequency Cepstrum Coefficients (MFCCs). The motivations and implementation of each step in the procedure were discussed. We’ve also argued the reasons behind the increasing popularity of filter banks compared to MFCCs. tl;dr: Use Mel-scaled filter banks if the machine learning algorithm is not susceptible to highly correlated input. Use MFCCs if the machine learning algorithm is susceptible to correlated input.]]></content>
      <tags>
        <tag>Speech Processing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Topic Identification]]></title>
    <url>%2F2019%2F03%2F11%2FTopic-Identification%2F</url>
    <content type="text"><![CDATA[topic identification 常规流程 speech is tokenized into words or phones by asr systems standard text-based processing techniques are appplied to the resulting tokenlizations Produce a vector representation for each spoken doument, typically a bag-of-wordds multinomial representation or a more compact vector give by probablilistic topic models topic id is performed on the spoken document representations by supervised training of classifiers such as bayesian classifiers and svm 解决的问题：a difficult and realistic scenario where the speech corppus of a test language is annotated only with a minimal number of topic labels, i.e no manual transcriptions or dictionaries for building an asr system are available Previous work: the cross-lingual phoneme recoginzers can prodeuc reasonable speech tokenizations 缺点：the performacne is highlyu dependent on the language and environmental condition(channel and noise) mismatch between the training and the test data Paper: topic identification for speech without asr Idea: focus on unsupervised approaches that operate directly on the speech of interest Raw acoustic feature based unsupervised term discovery is one such approach that aims to idnetify and cluster repeating word-like units across speech based around segmental dynamic time warping. UTD in 【NLP on spoken document without ASR】are limited since the acoustic features on which UTD is performed are produced by acoustic models trained from the transcribed speech of its evaluation corpus. 本文改进： UTD operates on language-independent speech representations extracted from multilingual bottleneck networks trained on languages other than the test language Another alternative to producing speech tokenizations without language dependecy is the model-based approach i.e. unsupervised learning of hidden markov model based phoneme0like units from untranscribed speech. 本文提出： the variational bayesian inference变分贝叶斯推断 based acoustic unit discovery(AUD) framework in 【variational inference for acoustic unit descovery】that allows parallelized large-scale training 在进行了speech 的tokenlization之后，所有的work are limited to using bag-of-words features as spoken document representations. UTD only identifies relatively long repeated terms(0.5 -1 sec) AUD/ASR enables full-coverage segmentation of continuous speech into a sequence of units/words and such a resulting temporal sequence 时间层序 enables another feature learning architecture based on CNN. 本文提出 Instead of treating the sequential tokens as a bag of acoustic units or words, the whole token sequence is encoded as concatenated continuous vectors, and followed by convolution and temporal pooling operations that capture the local and global dependecies. such continuous space feature extraction frameworks have been used in various language processing tasks like spoken language processing tasks like spoken language understanding and text classification 本文三个问题值得探究： 是否such a cnn-based framework can perform as well on noisy automatically discovered phoneme-like units as on otrhographic正射 words/characters pre-trained vectors of phoneme-like units from word2vec 是否provide superior perfomance to random initiallization as evidenced by the word-based tasks CNNs 是否 are still competitive in low-resource settings of hundreds to two-thousand training exemplars, rather than large/medium sized datasets UTD unsurpervised term discovery Aim to automatically identify and cluster repeated terms from speech . AUD based on variational inference, rather than the maximum likelihood training which may oversimplify the parameter estimations nor the Gibbs Sampling traing which is not amenable负责 to large scale applications Phone-loop model is formulated where each phoneme-line]]></content>
  </entry>
  <entry>
    <title><![CDATA[1-d peak and 2-d peak]]></title>
    <url>%2F2019%2F01%2F03%2F1-d-peak-and-2-d-peak%2F</url>
    <content type="text"><![CDATA[1-d peak leetcode 162. Find Peak Element A peak element is an element that is greater than its neighbors. Given an input array nums, where nums[i] ≠ nums[i+1], find a peak element and return its index. The array may contain multiple peaks, in that case return the index to any one of the peaks is fine. You may imagine that nums[-1] = nums[n] = -∞. Example 1:** 123Input: nums = [1,2,3,1]Output: 2Explanation: 3 is a peak element and your function should return the index number 2. O(n)时间复杂度的实现： 12345678910111213141516class Solution(object): def findPeakElement(self, nums): """ :type nums: List[int] :rtype: int """ if len(nums) == 1: return 0 if nums[0] &gt; nums[1]: return 0 i = 1 while i &lt;= len(nums) -1 and nums[i] &gt; nums[i-1] : i = i + 1 return i - 1 O(logn)的循环实现 123456789101112131415class Solution(object): def findPeakElement(self, nums): """ :type nums: List[int] :rtype: int """ start = 0 end = len(nums) - 1 while start &lt; end: mid = (start+end)/2 if nums[mid] &lt; nums[mid + 1]: start = mid + 1 else: end = mid return start 分析： 对于任意一个array都会存在peak 思想就是找到一个数组中间的值，如果这个值比左边的小，那么就可以保证左边一定有peak 如果这个值比右边的小，那么就可以保证右边一定有peak 如果这个值比两边大就说明，他自己就是peak 拓展2-d的peak 对于n行m列的数组 找到一个peak 方法就是 先找到中间的一列m/2 然后找到这一列的最大值 然后判断这个值如果小于左边的列，就对左边的数组进行从第一步开始的操作，如果小于右边的列，就对右边的数组进行相应的操作，如果都大于就可以直接返回了 T(n,m) = T(n,m/2) + O(n) –&gt;找到global maximum T(n,1) = O(n) T(n,m) = logm * O(n) O(nlogm)的实现方法]]></content>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bucket_sort]]></title>
    <url>%2F2019%2F01%2F03%2Fbucket-sort%2F</url>
    <content type="text"><![CDATA[bucket sort 总结 桶排序适用情况：输入需要排序的数据在一个范围内均匀分布 例如，对0.0到1.0均匀分布的浮点数进行排序 对于上述问题简单的方法是使用对比排序的方法，对于对比排序算法的下界(merge sort[归并排序]，heap sort[堆排序] ，quick-sort[快排])都是O(nlogn)的时间复杂度，也就是说他们不能比O(nlogn)更好 我们可以用线性的时间完成对上述问题进行排序吗？ 计数排序不能在这个问题中使用因为我们用每个参与排序的数值作为数组的index， 但是我们这里参与排序的是浮点数。 我们借用类似计数排序的思想，使用桶排序。 下面是桶排序的算法流程： 12345678910bucket_sort(A)1 n = A.length2 let B[0..n-1] be a new array3 for i = 0 to n-14 make B[i] an empty list5 for i = 1 to n 6 insert A[i] to list B[int(nA[i])]7 for i = 0 to n-18 sort listB[i] with insertion sort9 concatenate this lists B[0]..B[n-1]together in order 时间复杂度： 除了第8行之外，其他各行时间复杂度都是O(n) 分析第8行插入排序的时间复杂度： $$n_i$$代表桶B[i]的元素个数的随机变量 $$ T(n) = \Theta(n) +\sum_{i=0}^{n-1}O(n_i^2) $$ 通过对输入数据取期望，可以计算出期望的运行时间。 $$E[T(n)] = E[ \Theta(n) +\sum_{i=0}^{n-1}O(n_i^2)] =\Theta(n) +\sum_{i=0}^{n-1}O(E[n_i^2]) $$ $$E[n_i^2] = 2 - \frac{1}{n}$$ 证明： 我们定义指示变量：对所有i=0,1…,n-1和j = 1,2,…,n $$X_{ij} = I\{A[j]落入桶i\}$$ $$n_i = \sum_{j=1}^nX_{ij}$$ 所以 $$E[n_i^2] = E[\sum_{j=1}^n\sum_{k=1}^nX_{ij}X_{ik}] = E[\sum_{j=1}^nX_{ij}^2 + \sum_{1\le j\le n}\sum_{1\le k \le n ^{k\neq j }}X_{ij}X_{ik}] = \sum_{j=1}^nE[X_{ij}^2]+ \sum_{1\le j\le n}\sum_{1\le k \le n ^{k\neq j }}E[X_{ij}X_{ik}] $$ $$E[X_{ij}^2] = 1^2 \frac{1}{n} + 0^2 (1-\frac{1}{n}) = \frac{1}{n} $$ 当$k\neq j$的时候，随机变量$X_{ij}X_{ik}$是独立的，因此有$E[X_{ij}X_{ik}] = \frac{1}{n^2}$ $$E[n_i^2] = 1 + n(n-1)\frac{1}{n^2}=2-\frac{1}{n}$$ 所以$$ E[T(n)] = E[ \Theta(n) +\sum_{i=0}^{n-1}O(n_i^2)] =\Theta(n) +\sum_{i=0}^{n-1}O(E[n_i^2]) =\Theta(n) + n*(2-\frac{1}{n}) = \Theta(n) $$ 即使输入数据分布不服从均匀分布，桶排序依然可以在线性时间内完成，只要输入数据满足：所有桶大小的平方和与总的元素数成线性关系 例题： leetcode Top K Frequent Elements 前K个高频元素 将数组长度作为frequency的所有可能来进行桶排序]]></content>
      <tags>
        <tag>sort</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[text-classification]]></title>
    <url>%2F2018%2F12%2F24%2Ftext-classification%2F</url>
    <content type="text"><![CDATA[text classification ##Extract feature Model: bag of words In this model, a text is represneted as the bag(multiset) of its words, disregarding grammar and even word order but keeping mulitplicity. 也就是说使用词袋模型，不考虑语法，词序，却主要记录词的出现次数。 multiplicity：multiset {a, a, a, b, b, b}, a and b both have multiplicity 3. The list representation does not preserve the order of the words in the original sentences. This is just the main feature of the Bag-of-words model. However, term frequencies are not necessarily the best representation for the text. To address this problem, one of the most popular ways to normalize the term frequencies is to weight a term by the inverse of document frequency, or tf-idf. Tf-idf: term frequency-inverse document frequency numerical statisic that is intended to reflect how important a word is to a document in a. collection or corpus. The tf-idf increses proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust fot the fact that some words appear more frequently in general. Conceptually, we can view baf-of-word model as a speica.l case of the n-gram model, with n=1. For n&gt;1 the model is named w-shingling. Hash trickA fast and space-efficient way of vectorizing features. Turn arbitary features into indices in a vector or matrix. It works by applying a hash function to the features and using their hash values as indices directly, rather than lookin the dindices up in an associative array. 对于spam filtering的问题，这个过程的问题在于，随着训练集的增长，这些词典会占用大量的存储空间并且会逐渐增大。相反，如果词汇保持固定而不随着训练集的增加而增加，则对手可能试图发明不在所存储的词汇表中的新单词或拼写错误以绕过机器学习过滤器。 zipf’s lawThe frequency of any word is inversely proportional to its rank in the frequency table. Questions: augmented frequency, to prevent a bias towards longer documents, e.g. raw frequency divided by the raw frequency of the most occurring term in the document:]]></content>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mac同时使用网线和无线网]]></title>
    <url>%2F2018%2F12%2F10%2Fmac%E5%90%8C%E6%97%B6%E4%BD%BF%E7%94%A8%E7%BD%91%E7%BA%BF%E5%92%8C%E6%97%A0%E7%BA%BF%E7%BD%91%2F</url>
    <content type="text"><![CDATA[12345netstat -rn #查看网关的状态route get 0.0.0.0 #查看连接internet的网关sudo route -n delete default **.**.**.** #删除连接服务器的网关sudo route add -net 0.0.0.0 **.**.**.** #加入连接internet应该使用可以连接internet的网关sudo route add -net 服务器地址 可以连接服务器的网关]]></content>
      <tags>
        <tag>杂七杂八</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[杂七杂八整理]]></title>
    <url>%2F2018%2F11%2F29%2F%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[如何hexo+next添加algolia搜索 参考文章地址 jupyter需要加载安装好的虚拟环境 python编码规范 ​ 等号一般在赋值的时候两边都要加空格，但是当’=’用于指示关键字参数或默认参数值时, 不要在其两侧使用空格. 1Yes: def complex(real, imag=0.0): return magic(r=real, i=imag) ​ 避免在循环中用+和+=操作符来累加字符串. 由于字符串是不可变的, 这样做会创建不必要的临时对象, 并且导致二次方而不是线性的运行时间. 作为替代方案, 你可以将每个子串加入列表, 然后在循环结束后用 .join 连接列表. (也可以将每个子串写入一个 cStringIO.StringIO 缓存中.) 123456789Yes: x = a + b x = '%s, %s!' % (imperative, expletive) x = '&#123;&#125;, &#123;&#125;!'.format(imperative, expletive) x = 'name: %s; score: %d' % (name, n) x = 'name: &#123;&#125;; score: &#123;&#125;'.format(name, n)No: x = '%s%s' % (a, b) # use + in this case x = '&#123;&#125;&#123;&#125;'.format(a, b) # use + in this case x = imperative + ', ' + expletive + '!' x = 'name: ' + name + '; score: ' + str(n) 英语积累 Tensor and Function are interconnected and build up an acyclic graph]]></content>
      <tags>
        <tag>杂七杂八</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[word2vec]]></title>
    <url>%2F2018%2F11%2F27%2Fword2vec%2F</url>
    <content type="text"><![CDATA[word2vec 总结 wordmeaning 用dot product计算similartiy distributional similarity 和 distributed 概念区别 distributional similarity 是可以用这个词所搭配的前后文词语来了解这个词的含义 对比的是denotational的对词下定义的方式，也就是字典上解释这个词所用的方式，而上面所说的更像是我们用例句的方式来学习这个词 distributed是与one-hot vector相反的对word的表示方式，是dense vector main idea of word2vec Predict between every word and its context word two algorithm: skip-gram Predict context word given target (position independent) continuous bags of words predict target word from bag-of-words context two (moderately efficient) training methods Hierarchical softmax negative sampling Naive softmax Todo: But the trick is since we have a one hot target which is just predicted the word actually occurred. under this criteria the only thing that left in the cross entropy loss is the negative probablity of the true class. the next class course dot product 是similarity 的一种计算方式 softmax是一种将$R^V$空间的东西映射到概率分布空间的一种方法 1. exponentiate to make positive 2. nomalize to give probablility Softmax 起名字的原因，类似maximize 更大的东西 Q&amp;A: Actually, we have two words representation for the same word, one is when he is the central word, the other is when he is the context word. two vectors for each word Advantages: 1. make the math easier, because two representation are separated when you do the optimization rather than tied to each other 2. in practice emprically works a little bit better 思考： 在这个window里其实不考虑position的影响 intermissiong introduction: sentence embedding 可以用来做情感分析 希望用一个向量表示一个词汇 常用的方法是： use bag-of-words v(‘natural language processing’) = 1/3(v(‘natual’) + v(‘language’) +v(‘processing’)) 还会使用的方法recurrent nerual network, recursive nerual network and covolutional neural network pricenton paper work: simple unsurpervised network weighted bag-of-words and remove some special direction Step 1: 类似average的想法但是down weight to the frequent word p(w)是指这个词的频率，a是常数 $v_s = \frac{1}{|s|}\sum_{w\in s} \frac{a}{a+p(w)}v_w$ Step 2: computer the first principal componet u of {$v_s$} for all the sentence s in S do $v_s = v_s - uu^Tv_s$ Interpretation: given the sentence representation, emiting a single word 的概率是跟这个 word的频率以及与这个词与整个句子表达相近程度来决定的。 $\frac{dx^T}{dx} = I, \frac{dx^TA}{dx}=A, \frac{dAx}{dx}=A^T$ Cosine measure 需要除以两个vector的模，但是在这里面不需要除以因为对于一个vector的模特别大，相应的所有词都会乘上他使得大家都是平等的 stochastic gradient descent nerual network love noise]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch-faster-rcnn]]></title>
    <url>%2F2018%2F10%2F19%2Fpytorch-faster-rcnn%2F</url>
    <content type="text"><![CDATA[Image Pre-Processing The following pre-processing steps are applied to an image before it is sent through the network. These steps must be identical for both training and inference. The mean vector (, one number corresponding to each color channel) is not the mean of the pixel values in the current image but a configuration value that is identical across all training and test images. The default values for and parameters are 600 and 1200 respectively. Network OrganizationA R-CNN uses neural networks to solve two main problems: Identify promising regions (Region of Interest – ROI) in an input image that are likely to contain foreground objects Compute the object class probability distribution of each ROI – i.e., compute the probability that the ROI contains an object of a certain class. The user can then select the object class with the highest probability as the classification result. R-CNNs consist of three main types of networks: Head Region Proposal Network (RPN) Classification Network]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>faster-rcnn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Median of two sorted Arrays]]></title>
    <url>%2F2018%2F10%2F15%2FMedian-of-two-sorted-Arrays%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[dynamic connectivity]]></title>
    <url>%2F2018%2F09%2F12%2Fdynamic-connectivity%2F</url>
    <content type="text"><![CDATA[Quick-findSolve the dynamic connectivity problem -&gt; Quick Find so called eager algorithm贪心算法]]></content>
  </entry>
  <entry>
    <title><![CDATA[test_time_caffe]]></title>
    <url>%2F2018%2F09%2F05%2Ftest-time-caffe%2F</url>
    <content type="text"><![CDATA[Test time12345678#!/usr/bin/env shnow=$(date +"%Y%m%d_%H%M%S")log_name="test___"$now""srun -p Test --exclusive --gres=gpu:1 \ /mnt/lustre/share/sensephoto/caffes/caffe/build/tools/caffe time \ --model=$1 \ --iterations=1000 \ 2&gt;&amp;1|tee $log_name.log &amp;]]></content>
      <categories>
        <category>caffe</category>
      </categories>
      <tags>
        <tag>caffe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[stereo camera]]></title>
    <url>%2F2018%2F08%2F22%2Fstereo-camera%2F</url>
    <content type="text"><![CDATA[Stereo vision basicspinhole camera model]]></content>
      <tags>
        <tag>stereo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Depth-camera]]></title>
    <url>%2F2018%2F08%2F14%2FDepth-camera%2F</url>
    <content type="text"><![CDATA[立体视觉的基本知识 stereoscopic vision ##Why depth? 深度相机是添加一个全新的信息通道，每个像素的距离。 ##立体视觉 立体视觉的深度信息提取是受人类双目视觉的启发。它依赖于两个平行的视觉窗口，并通过估计左右图像中匹配的关键点的差异来计算深度。]]></content>
      <tags>
        <tag>Depth-camera</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[librealsense]]></title>
    <url>%2F2018%2F08%2F01%2Flibrealsense%2F</url>
    <content type="text"><![CDATA[Depth from Stereo Using this tutorial you will learn the basics of stereoscopic vision, including block-matching, calibration and rectification, depth from stereo using opencv, passive vs. active stereo and relation to structured light. Why Depth?Regular consumer web-cams offer streams of RGB data within the visible spectrum. This data can be used for object recognition and tracking, as well as some basic scene understanding. Even with machine learning grasping the exact dimensions of physical objects is a very hard problem This is where depth cameras come-in. The goal of a depth camera is to add a brand-new channel of information, with distance to every pixel.This new channel can be used just like the rest (for training and image processing) but also for measurement and scene reconstruction. n VisionDepth from Stereo is a classic computer vision algorithm inspired by human binocular vision system. It relies on two parallel view-ports and calculates depth by estimating disparities between matching key-points in the left and right images: Depth from Stereo algorithm finds disparity by matching blocks in left and right images Most naive implementation of this idea is the SSD (Sum of Squared Differences) block-matching algorithm: 123456789101112131415161718192021222324import numpyfx = 942.8 # lense focal lengthbaseline = 54.8 # distance in mm between the two camerasdisparities = 64 # num of disparities to considerblock = 15 # block size to matchunits = 0.001 # depth unitsfor i in xrange(block, left.shape[0] - block - 1): for j in xrange(block + disparities, left.shape[1] - block - 1): ssd = numpy.empty([disparities, 1]) # calc SSD at all possible disparities l = left[(i - block):(i + block), (j - block):(j + block)] for d in xrange(0, disparities): r = right[(i - block):(i + block), (j - d - block):(j - d + block)] ssd[d] = numpy.sum((l[:,:]-r[:,:])**2) # select the best match disparity[i, j] = numpy.argmin(ssd)# Convert disparity to depthdepth = np.zeros(shape=left.shape).astype(float)depth[disparity &gt; 0] = (fx * baseline) / (units * disparity[disparity &gt; 0]) Rectified image pair used as input to the algorithm Depth map produced by the naive SSD block-matching implementation Point-cloud reconstructed using SSD block-matching There are several challenges that any actual product has to overcome: Ensuring that the images are in fact coming from two parallel views Filtering out bad pixels where matching failed due to occlusion Expanding the range of generated disparities from fixed set of integers to achieve sub-pixel accuracy D435This document describes the projection mathematics relating the images provided by the RealSense depth devices to their associated 3D coordinate systems, as well as the relationships between those coordinate systems. These facilities are mathematically equivalent to those provided by previous APIs and SDKs, but may use slightly different phrasing of coefficients and formulas. Table of Contents Pixel Coordinates Point Coordinates Intrinsic Camera Parameters Distortion Models Extrinsic Camera Parameters Depth Image Formats Processing Blocks Helpers Point Cloud Frame Alignment Appendix: Model Specific Details SR300 D400 Pixel CoordinatesEach stream of images provided by this SDK is associated with a separate 2D coordinate space, specified in pixels, with the coordinate [0,0] referring to the center of the top left pixel in the image, and [w-1,h-1] referring to the center of the bottom right pixel in an image containing exactly w columns and h rows. That is, from the perspective of the camera, the x-axis points to the right and the y-axis points down. Coordinates within this space are referred to as “pixel coordinates”, and are used to index into images to find the content of particular pixels. 左上角是[0,0],右下角是[w-1,h-1] Point CoordinatesEach stream of images provided by this SDK is also associated with a separate 3D coordinate space, specified in meters, with the coordinate [0,0,0] referring to the center of the physical imager. Within this space, the positive x-axis points to the right, the positive y-axis points down, and the positive z-axis points forward. Coordinates within this space are referred to as “points”, and are used to describe locations within 3D space that might be visible within a particular image. Intrinsic Camera ParametersThe relationship between a stream’s 2D and 3D coordinate systems is described by its intrinsic camera parameters, contained in the rs2_intrinsics struct. Each model of RealSense device is somewhat different, and the rs2_intrinsics struct must be capable of describing the images produced by all of them. The basic set of assumptions is described below: Images may be of arbitrary size The width and height fields describe the number of rows and columns in the image, respectively The field of view of an image may vary The fx and fy fields describe the focal length of the image, as a multiple of pixel width and height The pixels of an image are not necessarily square The fx and fy fields are allowed to be different (though they are commonly close) The center of projection is not necessarily the center of the image The ppx and ppy fields describe the pixel coordinates of the principal point (center of projection) The image may contain distortion The model field describes which of several supported distortion models was used to calibrate the image, and the coeffs field provides an array of up to five coefficients describing the distortion model Knowing the intrinsic camera parameters of an images allows you to carry out two fundamental mapping operations. Projection Projection takes a point from a stream’s 3D coordinate space, and maps it to a 2D pixel location on that stream’s images. It is provided by the header-only function rs2_project_point_to_pixel(...). Deprojection Deprojection takes a 2D pixel location on a stream’s images, as well as a depth, specified in meters, and maps it to a 3D point location within the stream’s associated 3D coordinate space. It is provided by the header-only function rs2_deproject_pixel_to_point(...). Intrinsic parameters can be retrieved from any rs2::video_stream_profile object via a call to get_intrinsics(). (See example here) Distortion ModelsBased on the design of each model of RealSense device, the different streams may be exposed via different distortion models. None An image has no distortion, as though produced by an idealized pinhole camera. This is typically the result of some hardware or software algorithm undistorting an image produced by a physical imager, but may simply indicate that the image was derived from some other image or images which were already undistorted. Images with no distortion have closed-form formulas for both projection and deprojection, and can be used with both rs2_project_point_to_pixel(...) and rs2_deproject_pixel_to_point(...). Modified Brown-Conrady Distortion An image is distorted, and has been calibrated according to a variation of the Brown-Conrady Distortion model. This model provides a closed-form formula to map from undistorted points to distorted points, while mapping in the other direction requires iteration or lookup tables. Therefore, images with Modified Brown-Conrady Distortion are being undistorted when calling rs2_project_point_to_pixel(...). This model is used by the RealSense D415’s color image stream. Inverse Brown-Conrady Distortion An image is distorted, and has been calibrated according to the inverse of the Brown-Conrady Distortion model. This model provides a closed-form formula to map from distorted points to undistored points, while mapping in the other direction requires iteration or lookup tables. Therefore, images with Inverse Brown-Conrady Distortion are being undistorted when calling rs2_deproject_pixel_to_point(...). This model is used by the RealSense SR300’s depth and infrared image streams. Although it is inconvenient that projection and deprojection cannot always be applied to an image, the inconvenience is minimized by the fact that RealSense devices always support deprojection from depth images, and always support projection to color images. Therefore, it is always possible to map a depth image into a set of 3D points (a point cloud), and it is always possible to discover where a 3D object would appear on the color image. Extrinsic Camera ParametersThe 3D coordinate systems of each stream may in general be distinct. For instance, it is common for depth to be generated from one or more infrared imagers, while the color stream is provided by a separate color imager. The relationship between the separate 3D coordinate systems of separate streams is described by their extrinsic parameters, contained in the rs2_extrinsics struct. The basic set of assumptions is described below: Imagers may be in separate locations, but are rigidly mounted on the same physical device The translation field contains the 3D translation between the imager’s physical positions, specified in meters Imagers may be oriented differently, but are rigidly mounted on the same physical device The rotation field contains a 3x3 orthonormal rotation matrix between the imager’s physical orientations All 3D coordinate systems are specified in meters There is no need for any sort of scaling in the transformation between two coordinate systems All coordinate systems are right handed and have an orthogonal basis There is no need for any sort of mirroring/skewing in the transformation between two coordinate systems Knowing the extrinsic parameters between two streams allows you to transform points from one coordinate space to another, which can be done by calling rs2_transform_point_to_point(...). This operation is defined as a standard affine transformation using a 3x3 rotation matrix and a 3-component translation vector. Extrinsic parameters can be retrieved via a call to rs2_get_extrinsics(...) between any two streams which are supported by the device, or using a rs2::stream_profile object via get_extrinsics_to(...) (see example here). One does not need to enable any streams beforehand, the device extrinsics are assumed to be independent of the content of the streams’ images and constant for a given device for the lifetime of the program. Depth Image FormatsAs mentioned above, mapping from 2D pixel coordinates to 3D point coordinates via the rs2_intrinsics structure and the rs2_deproject_pixel_to_point(...) function requires knowledge of the depth of that pixel in meters. Certain pixel formats, exposed by this SDK, contain per-pixel depth information, and can be immediately used with this function. Other images do not contain per-pixel depth information, and thus would typically be projected into instead of deprojected from. RS2_FORMAT_Z16 (Under rs2_format) Depth is stored as one unsigned 16-bit integer per pixel, mapped linearly to depth in camera-specific units. The distance, in meters, corresponding to one integer increment in depth values can be queried via rs2_get_depth_scale(...) or using a rs2::depth_sensor via get_depth_scale() (see example here). The following shows how to retrieve the depth of a pixel in meters: Using C API: 123const float scale = rs2_get_depth_scale(sensor, NULL);const uint16_t * image = (const uint16_t *)rs2_get_frame_data(frame, NULL);float depth_in_meters = scale * image[pixel_index]; Using C++ API: 12rs2::depth_frame dpt_frame = frame.as&lt;rs2::depth_frame&gt;();float pixel_distance_in_meters = dpt_frame.get_distance(x,y); using Python API: 12dpt_frame = pipe.wait_for_frames().get_depth_frame().as_depth_frame()pixel_distance_in_meters = dpt_frame.get_distance(x,y) If a device fails to determine the depth of a given image pixel, a value of zero will be stored in the depth image. This is a reasonable sentinel for “no depth” because all pixels with a depth of zero would correspond to the same physical location, the location of the imager itself. The default scale of an SR300 device is 1/32th of a millimeter, allowing for a maximum expressive range of two meters. However, the scale is encoded into the camera’s calibration information, potentially allowing for long-range models to use a different scaling factor. The default scale of a D400 device is one millimeter, allowing for a maximum expressive range of ~65 meters. The depth scale can be modified by calling rs2_set_option(...) with RS2_OPTION_DEPTH_UNITS, which specifies the number of meters per one increment of depth. 0.001 would indicate millimeter scale, while 0.01 would indicate centimeter scale. Processing Blocks HelpersThe SDK provides two main processing blocks related to image projecting: Point Cloud Frame Alignment Point CloudAs part of the API we offer a processing block for creating a point cloud and corresponding texture mapping from depth and color frames. The point cloud created from a depth image is a set of points in the 3D coordinate system of the depth stream. The following demonstrates how to create a point cloud object: Using C API: 123456789rs2_frame_queue* q = rs2_create_frame_queue(1, NULL);rs2_processing_block* pc = rs2_create_pointcloud(NULL);rs2_start_processing_queue(pc, q, NULL);rs2_process_frame(pc, depth_frame, NULL);rs2_frame* points = rs2_wait_for_frame(q, 5000, NULL);const rs2_stream_profile* sp = rs2_get_frame_stream_profile(color_frame, NULL);rs2_get_stream_profile_data(sp, ..., &amp;uid, ..., NULL);rs2_set_option((rs2_options*)pc, RS2_OPTION_TEXTURE_SOURCE, uid, NULL);rs2_process_frame(pc, color_frame, NULL); Using C++ API: 123rs2::pointcloud pc;rs2::points points = pc.calculate(depth_frame);pc.map_to(color_frame); using Python API: 1234import pyrealsense2 as rspc = rs.pointcloud()points = pc.calculate(depth_frame)pc.map_to(color_frame) For additional examples, see examples/pointcloud. In addition, an example for integrating with PCL (Point Cloud Library) is available here. Frame AlignmentUsually when dealing with color and depth images, mapping each pixel from one image to the other is desired. The SDK offers a processing block for aligning the image to one another, producing a set of frames that share the same resolution and allow for easy mapping of pixels. The following demonstrates how to create an align object: Using C API: 123456789rs2_processing_block* align = rs2_create_align(RS2_STREAM_COLOR, NULL);rs2_frame_queue* q = rs2_create_frame_queue(1, NULL);rs2_start_processing_queue(align, q, NULL);rs2_process_frame(align, depth_and_color_frameset, NULL);rs2_frame* aligned_frames = rs2_wait_for_frame(q, 5000, NULL);for (i = 0; i &lt; rs2_get_frame_points_count(aligned_frames, NULL); i++)&#123; rs2_frame* aligned_frame = rs2_extract_frame(aligned_frames, i, NULL);&#125; Using C++ API: 1234rs2::align align(RS2_STREAM_COLOR);auto aligned_frames = align.process(depth_and_color_frameset);rs2::video_frame color_frame = aligned_frames.first(RS2_STREAM_COLOR);rs2::depth_frame aligned_depth_frame = aligned_frames.get_depth_frame(); using Python API: 12345import pyrealsense2 as rsalign = rs.align(rs.stream.color)aligned_frames = align.proccess(depth_and_color_frameset)color_frame = aligned_frames.first(rs.stream.color)aligned_depth_frame = aligned_frames.get_depth_frame() For additional examples, see examples/align and align-depth2color.py. Appendix: Model Specific DetailsIt is not necessary to know which model of RealSense device is plugged in to successfully make use of the projection capabilities of this SDK. However, developers can take advantage of certain known properties of given devices. SR300 Depth images are always pixel-aligned with infrared images The depth and infrared images have identical intrinsics The depth and infrared images will always use the Inverse Brown-Conrady distortion model The extrinsic transformation between depth and infrared is the identity transform Pixel coordinates can be used interchangeably between these two streams Color images have no distortion When projecting to the color image on these devices, the distortion step can be skipped entirely D400-Series Left and right infrared images are rectified by default (Y16 format is not) The two infrared streams have identical intrinsics The two infrared streams have no distortion There is no rotation between left and right infrared images (identity matrix) There is translation on only one axis between left and right infrared images (translation[1] and translation[2] are zero) Therefore, the y component of pixel coordinates can be used interchangeably between these two streams Stereo disparity is related to depth via an inverse linear relationship, and the distance of a point which registers a disparity of 1 can be queried via 1 / rs2_get_depth_scale(...). The following shows how to retrieve the depth of a pixel in meters: 123const float scale = rs2_get_depth_scale(sensor, NULL);const uint16_t * image = (const uint16_t *)rs2_get_frame_data(frame, NULL);float depth_in_meters = 1 / (scale * image[pixel_index]); Unlike RS2_FORMAT_Z16, a disparity value of zero is meaningful. A stereo match with zero disparity will occur for objects “at infinity”, objects which are so far away that the parallax between the two imagers is negligible. By contrast, there is a maximum possible disparity. Disparity is currently only available on the D400. Controlling disparity can be modified using the advanced_mode controls.]]></content>
      <categories>
        <category>openark</category>
      </categories>
      <tags>
        <tag>realsense open_ark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenCv_learn]]></title>
    <url>%2F2018%2F07%2F30%2FOpenCv-learn%2F</url>
    <content type="text"><![CDATA[Mat的理解 Mat的好处是可以自动的释放内存 Mat是一个基础的类有两个部分 矩阵头（包括矩阵大小，存储方式，存储地址等信息） 能指向存储像素信息的矩阵 Mat is basically a class with two data parts: the matrix header (containing information such as the size of the matrix, the method used for storing, at which address is the matrix stored, and so on) and a pointer to the matrix containing the pixel values (taking any dimensionality depending on the method chosen for storing)? opencv考虑减少对大图片不必要的拷贝，opencv使用reference counting system。每一个mat对象都有自己的矩阵头，但是实际的图片矩阵可以被多个对象的指针共享，即多个指针指向同一个地址。copy操作只会拷贝矩阵头和指向的指针不会拷贝数据本身。 123456Mat A, C; // creates just the header partsA = imread(argv[1], IMREAD_COLOR); // here we'll know the method used (allocate matrix)Mat B(A); // Use the copy constructorC = A; // Assignment operator A，B，C的header是不同的，但是指向的是同一块内存，通过指针修改内存的数值会导致三个的存储的值改变。 可以通过建立新的hearder来产生ROI。 12Mat D (A, Rect(10, 10, 100, 100) ); // using a rectangleMat E = A(Range::all(), Range(1,3)); // using row and column boundaries 但所有的对象都不用了之后，内存就会被释放。利用的reference counting mechanism。 如果希望拷贝数据本身的话，opencv提供clone()和copyTo()这两个函数 123Mat F = A.clone();Mat G;A.copyTo(G); Summary: Output image allocation for OpenCV functions is automatic (unless specified otherwise). You do not need to think about memory management with OpenCVs C++ interface. The assignment operator and the copy constructor only copies the header. The underlying matrix of an image may be copied using the clone() and copyTo() functions.]]></content>
      <categories>
        <category>openark</category>
      </categories>
      <tags>
        <tag>C++, openark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Const_C++]]></title>
    <url>%2F2018%2F07%2F30%2FConst-C%2F</url>
    <content type="text"><![CDATA[Const c++的使用总结 12345const std::vector&lt;Hand::Ptr&gt; &amp; HandDetector::getHands() const &#123; return hands; &#125;//const函数重载 基本知识 const修饰常量时，定义时必须初始化 对于类中的const成员必须通过初始化列表进行初始化 123456789101112131415class A&#123; public: A(int i); void print(); const int &amp;r; private: const int a; static const int b;&#125;;const int A::b=10;A::A(int i):a(i),r(a)&#123; &#125; Const 默认在文件中为局部变量，如果需要在别的文件中访问需要显示的定义为外部变量，非const的变量默认为外部变量 很有趣的地方： 1234567891011#include &lt;iostream&gt;int main(int argc, const char * argv[]) &#123; // insert code here... double a = 3.14; const int &amp;ri = a; // Non-const lvalue reference to type 'int' cannot bind to a value of unrelated type 'double' int &amp;b = a; std::cout &lt;&lt; "Hello, World!\n"; return 0;&#125; 非const引用只能绑定到与该引用相同类型的对象。 const引用则可以绑定到不同但相关的类型的对象或绑定到右值。 原因在于编译器在编译的时候，对于引用存放的是一个对象的地址，对于不可寻址的值，如文字常量以及不同类型的对象，编译器为了实现引用，必须生成一个临时对象，引用实际上指向该对象，但用户不能访问它。ri是const所以没有办法修改的ri的赋值，但是对于b一旦修改了b的值我们希望同时修改a的值，但是由于编译器其实修改的是临时对象temp的值，而不是a的值，所以会造成误解，所以在编译的时候回报错。 const对象的动态数组 1234const int *temp = new const int[100]();//const的修饰的数组必须初始化，所以使用这种方式可以zero initial整个数组const std::string *temp = new std::string[2];//会调用string类的默认构造函数初始化//c++允许定义类类型的const数组，但是必须提供默认构造函数 指针和const修饰符 指向const的指针123456789101112const double *cptr;// cptr是一个指向double类型的const对象的指针，cptr可以指向不同的const对象，但是不能通过cptr修改对象的值。*ctpn = 42;//错误，不能修改值const double pi = 3.14；double *ptr = $pi; //错误，不能把一个const的对象的地址赋给一个普通的非const指针，因为这样可以修改const的值const double *ctpr = &amp;pi;//可以的//允许把非const对象的值赋给const对象的指针 但是不能用const对象的指针来修改其值，修改方法需要借助别的非const指针double dval = 3.13;cptr = &amp;dval;double *ptr = &amp;dval;*ptr =3.14;cout&lt;&lt; *cptr;//结果是3.14 自以为指向const的指针，偷偷修改它也不知道 常指针 本身的值不能修改，与其他const量一样，需要在定义的时候初始化。 12int a = 0;int *const ptr = &amp;a;//能不能修改指向对象的值，取决于指向对象本身的类型 函数和const限定符的关系 类中的const成员函数 在一个类中，任何不会修改数据成员的函数都应该声明为const类型。如果在编写const成员函数时，不慎修改了数据成员，或者调用了其它非const成员函数，编译器将指出错误，这无疑会提高程序的健壮性。使用const关键字进行说明的成员函数，称为常成员函数。只有常成员函数才有资格操作常量或常对象，没有使用const关键字说明的成员函数不能用来操作常对象。常成员函数说明格式如下： &lt;类型说明符&gt; &lt;函数名&gt; (&lt;参数表&gt;) const； 12345678910111213141516class tempstack&#123;private: int m_num; int m_data[100];public: void Push(int elem); int Pop(void); int GetCount(void) const;&#125;;int tempstack::GetCount(void) const&#123; //++m_num;//编译错误Cannot assign to non-static data member within const member function 'GetCount' //Pop();//Member function 'Pop' not viable: 'this' argument has type 'const tempstack', but function is not marked const return m_num;&#125; 函数重载 123456789101112131415161718192021222324252627282930#include &lt;iostream&gt;class R&#123;public: R(int r1,int r2) &#123; R1 = r1; R2 = r2; &#125; void print(); void print() const;private: int R1,R2;&#125;;void R::print()&#123; std::cout&lt;&lt;R1;&#125;void R::print() const&#123; std::cout&lt;&lt;R2;&#125;int main(int argc, const char * argv[]) &#123; R a(1,2); a.print();//1 const R b(3,4); b.print();//4 return 0;&#125; const 对象默认调用const成员函数]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[handDetect]]></title>
    <url>%2F2018%2F07%2F27%2FhandDetect%2F</url>
    <content type="text"><![CDATA[使用RealSense进行手势识别 原理 Compute a depth map The depth map is constructed by analyzing a speckle pattern of infrared laser light 具体建立结构光的原理是受专利保护的 Structured light general principle: project a known pattern onto the scene and infer depth from the deformation of that pattern The Kinect combines structured light with two classic computer vision techniques: depth from focus, and depth from stereo Depth from focus uses the principle that stuff that is more blurry is further away 越远的物体就会更模糊 The astigmatic lens causes a projected circle to become an ellipse whose orientation depends on depth 像散透镜使投影圆变成椭圆，其取向取决于深度 Depth from stereo uses parallax 2.infer body postion 代码梳理12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061// C++ Libraries#include &lt;stdio.h&gt;#include &lt;iostream&gt;#include &lt;string&gt;// OpenCV Libraries#include &lt;opencv/cxcore.h&gt;#include "opencv2/highgui/highgui.hpp"// OpenARK Libraries#include "Core.h"#include "RS2Camera.h"#include "Visualizer.h"using namespace ark;int main() &#123; // change the SR300Camera to other type if needed ark::DepthCamera::Ptr camera = std::make_shared&lt;ark::RS2Camera&gt;(); ark::DetectionParams::Ptr params = ark::DetectionParams::create(); //ark::HandDetector handDetector(true, params); PlaneDetector::Ptr planeDetector = std::make_shared&lt;PlaneDetector&gt;(); HandDetector::Ptr handDetector = std::make_shared&lt;HandDetector&gt;(planeDetector); //ark::HandDetector handDetector(false, params); // change first argument to true to remove planes in frame camera-&gt;beginCapture(); //1.step int frame = 0; while (true) &#123; // Show image cv::Mat xyzVisual; ark::Visualizer::visualizeXYZMap(camera-&gt;getXYZMap(), xyzVisual); cv::imshow("XYZ Map", xyzVisual); // Find hands handDetector-&gt;update(*camera); auto hands = handDetector-&gt;getHands(); if (!hands.empty()) &#123; ark::Hand::Ptr hand = hands[0]; // Show the hand cv::Mat visual = camera-&gt;getXYZMap().clone(); ark::Visualizer::visualizeHand(visual, visual, hand.get(), hand-&gt;getSVMConfidence()); cv::imshow("Result", visual); &#125; /**** Start: Loop Break Condition ****/ int c = cv::waitKey(1); if (c == 'q' || c == 'Q' || c == 27) &#123; break; &#125; /**** End: Loop Break Condition ****/ ++frame; &#125; return 0;&#125; 和硬件连接的部分capture1234567891011121314151617181920 /** * Begin capturing frames continuously from this camera on a parallel thread, * capped at a certain maximum FPS. * WARNING: throws an error if capture already started. * @param fps_cap maximum FPS of capture (-1 to disable) * @param removeNoise if true, performs noise removal on the depth image after retrieving it * @see endCapture * @see isCapturing */ void beginCapture(int fps_cap = -1, bool remove_noise = true);void DepthCamera::beginCapture(int fps_cap, bool remove_noise)&#123; ASSERT(captureInterrupt == true, "beginCapture: already capturing from this camera"); captureInterrupt = false; std::thread thd(&amp;DepthCamera::captureThreadingHelper, this, fps_cap, &amp;captureInterrupt, remove_noise); thd.detach();&#125; c++多线程 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960// C++ Libraries#include &lt;stdio.h&gt;#include &lt;iostream&gt;#include &lt;string&gt;// OpenCV Libraries#include &lt;opencv/cxcore.h&gt;#include &quot;opencv2/highgui/highgui.hpp&quot;// OpenARK Libraries#include &quot;Core.h&quot;#include &quot;RS2Camera.h&quot;#include &quot;Visualizer.h&quot;using namespace ark;int main() &#123; // change the SR300Camera to other type if needed ark::DepthCamera::Ptr camera = std::make_shared&lt;ark::RS2Camera&gt;(); ark::DetectionParams::Ptr params = ark::DetectionParams::create(); //ark::HandDetector handDetector(true, params); PlaneDetector::Ptr planeDetector = std::make_shared&lt;PlaneDetector&gt;(); HandDetector::Ptr handDetector = std::make_shared&lt;HandDetector&gt;(planeDetector); //ark::HandDetector handDetector(false, params); // change first argument to true to remove planes in frame camera-&gt;beginCapture(); int frame = 0; while (true) &#123; // Show image cv::Mat xyzVisual; ark::Visualizer::visualizeXYZMap(camera-&gt;getXYZMap(), xyzVisual); cv::imshow(&quot;XYZ Map&quot;, xyzVisual); // Find hands handDetector-&gt;update(*camera); auto hands = handDetector-&gt;getHands(); if (!hands.empty()) &#123; ark::Hand::Ptr hand = hands[0]; // Show the hand cv::Mat visual = camera-&gt;getXYZMap().clone(); ark::Visualizer::visualizeHand(visual, visual, hand.get(), hand-&gt;getSVMConfidence()); cv::imshow(&quot;Result&quot;, visual); &#125; /**** Start: Loop Break Condition ****/ int c = cv::waitKey(1); if (c == &apos;q&apos; || c == &apos;Q&apos; || c == 27) &#123; break; &#125; /**** End: Loop Break Condition ****/ ++frame; &#125; return 0;&#125;]]></content>
      <categories>
        <category>handDetect</category>
      </categories>
      <tags>
        <tag>realsense</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[faster-rcnn]]></title>
    <url>%2F2018%2F06%2F15%2Ffaster-rcnn%2F</url>
    <content type="text"><![CDATA[Faster-rcnn 理解 Region Proposal NetworksA Region Proposal Network (RPN) takes an image (of any size) as input and outputs a set of rectangular object proposals, each with an objectness score.3 We model this process with a fully convolutional network [7], which we describe in this section. Because our ultimate goal is to share computation with a Fast R-CNN object detection network [2], we assume that both nets share a common set of convolutional layers. In our experiments, we investigate the Zeiler and Fergus model [32] (ZF), which has 5 shareable convolutional layers and the Simonyan and Zisserman model [3] (VGG-16), which has 13 shareable convolutional layers. To generate region proposals, we slide a small network over the convolutional feature map output by the last shared convolutional layer. This small network takes as input an n × n spatial window of the input convolutional feature map. Each sliding window is mapped to a lower-dimensional feature (256-d for ZF and 512-d for VGG, with ReLU [33] following). This feature is fed into two sibling fullyconnected layers—a box-regression layer (reg) and a box-classification layer (cls). We use n = 3 in this paper, noting that the effective receptive field on the input image is large (171 and 228 pixels for ZF and VGG, respectively). This mini-network is illustrated at a single position in Figure 3 (left). Note that because the mini-network operates in a sliding-window fashion, the fully-connected layers are shared across all spatial locations. This architecture is naturally implemented with an n×n convolutional layer followed by two sibling 1 × 1 convolutional layers (for reg and cls, respectively). The region proposal layer consists of a Region Proposal Network and three layers – Proposal Layer, Anchor Target Layer and Proposal Target Layer.]]></content>
      <categories>
        <category>faster-rcnn</category>
      </categories>
      <tags>
        <tag>faster-rcnn object-detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[install_tmux]]></title>
    <url>%2F2018%2F06%2F11%2Finstall-tmux%2F</url>
    <content type="text"><![CDATA[如何在没有root权限的情况下安装tmux 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#!/bin/bash# tmux will be installed in $INSTALL_DIR/local/bin.# It's assumed that wget and a C/C++ compiler are installed.# exit on errorset -eTMUX_VERSION=2.7INSTALL_DIR=/home/dongzx13/tools# create our directoriesmkdir -p $INSTALL_DIR/local $INSTALL_DIR/tmux_tmpcp ncurses-5.9.tar.gz $INSTALL_DIR/tmux_tmpcp libevent-2.1.8-stable.tar.gz $INSTALL_DIR/tmux_tmpcp tmux-2.7.tar.gz $INSTALL_DIR/tmux_tmpcd $INSTALL_DIR/tmux_tmp# download source files for tmux, libevent, and ncurses# extract files, configure, and compile############# libevent #############tar xvzf libevent-2.1.8-stable.tar.gzcd libevent-2.1.8-stable./configure --prefix=$INSTALL_DIR/local --disable-sharedmakemake installcd ..############# ncurses #############if [[ $(fs --version) =~ "afs" ]] &amp;&amp; fs whereis "$HOME/local" ; then NCURSES_OPTION=" --enable-symlinks"else NCURSES_OPTION=""fitar xvzf ncurses-5.9.tar.gzcd ncurses-5.9./configure --prefix=$INSTALL_DIR/local $NCURSES_OPTIONmakemake installcd ..############# tmux #############tar xvzf tmux-$&#123;TMUX_VERSION&#125;.tar.gzcd tmux-$&#123;TMUX_VERSION&#125;sh ./autogen.sh./configure CFLAGS="-I$INSTALL_DIR/local/include -I$INSTALL_DIR/local/include/ncurses" LDFLAGS="-L$INSTALL_DIR/local/lib -L$INSTALL_DIR/local/include/ncurses -L$INSTALL_DIR/local/include"CPPFLAGS="-I$INSTALL_DIR/local/include -I$INSTALL_DIR/local/include/ncurses" LDFLAGS="-static -L$INSTALL_DIR/local/include -L$INSTALL_DIR/local/include/ncurses -L$INSTALL_DIR/local/lib" makecp tmux $INSTALL_DIR/local/bincd ..# cleanuprm -rf $INSTALL_DIR/tmux_tmpecho "$INSTALL_DIR/local/bin/tmux is now available. You can optionally add $INSTALL_DIR/local/bin to your PATH." 参考: no root to install tmux]]></content>
      <tags>
        <tag>install_software_no_root</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ctpn]]></title>
    <url>%2F2018%2F06%2F09%2Fctpn%2F</url>
    <content type="text"><![CDATA[准备数据集 将数据集转化为pascal voc的recognition格式 代码: prepare_img Annotations 标注 0368.xml … ImageSets Layout Main trainval.txt这个文件中的放的是图像文件的名字去掉&lt;.jpg&gt; Segmentation JPEGImages 图像文件 0368.jpg … SegmentationClass 用来分割 SegmentationObject 用来分割 如何使用这个数据集 代码: datasets ds_utils.py 代码解读: factory.py 工厂类 1lambda split=split, year=year: pascal_voc(split, year) 解读: lambda 函数 imdb.py assert语句用来声明某个条件是真的 pascal_voc.py]]></content>
      <categories>
        <category>text-detection</category>
      </categories>
      <tags>
        <tag>text-detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AdaBoost]]></title>
    <url>%2F2018%2F06%2F03%2Fadaboost%2F</url>
    <content type="text"><![CDATA[AdaBoost, is short for adaptive boosting. Boosting can give good results even if the base classifiers have a performance that is only slightly better than random, and hence sometimes the base classifiers are known as weak learners. Originally designed for solving classification problems, boosting can also be extended to regression. Adaboost算法流程 初始化每一个数据的权重系数 weighting coefficients {$w_n$}: $w_n^{(1)}$ = 1/N for n = 1,…,N. For m = 1,…,M: 在训练数据上训练 一个分类器classifier $y_m(x)$ ,使得加权的错误率最低:$$J_m = \sum_{n=1}^Nw_n^{(m)}I(y_m( {\textbf x_n})\neq t_n)$$ ​ 其中$I(y_m({\textbf x_n})\neq t_n)$是指示函数,当$y_m({\textbf x_n})\neq t_n$时,这个函数为1,其余的时候为0. 计算下面的式子:$$\epsilon_{m}=\frac{\sum_{n=1}^{N}w_{n}^{(m)}I(y_{m}(\textbf x_n)\ne t_{n})}{\sum_{n=1}^{N}w_{n}^{(m)}}$$利用上面的式子去计算$\alpha_m​$:$$\alpha_{m}=ln\{\frac{1-\epsilon_{m}}{\epsilon_{m}}\}$$ 更新权重系数$$w_{n}^{(m+1)}=w_{n}^{(m)}exp[ \alpha_{m}I(y_{m}(\textbf x_n)\neq t_{n}) ]$$ 利用最后的模型做出预测:$$Y_M(\textbf {x}) = sign(\sum_{m=1}^M\alpha_my_m(\textbf x))$$ 理论解释—对指数误差函数的顺序最小化指数误差函数:$$E=\sum_{n=1}^{N}exp[ -t_{n}f_{m}(\textbf x_n) ]$$ 其中$f_{m}(\textbf x)$可以看作是基础分类器$y_{l}(\textbf x)$的线性组合:$$f_{m}(\textbf x)=\frac{1}{2}\sum_{l=1}^{m}\alpha_{l}y_{l}(\textbf x)$$ ​ 且$t_n\in\{ -1,1\}$是训练的目标值.我们的目标是根据权重系数$\alpha_{l}$ 以及$y_l(\textbf x)$ 来最小化指数误差函数E. 在这里我们假定基础的分类器$y_{1}(\textbf{x}), …, y_{m-1}(\textbf{x})$ 以及他们的系数$\alpha_{1}, …, \alpha_{m-1}$是固定的,我们不对误差函数做全局的最小化,而是仅仅针对$\alpha_{m}$ and $y_{m}(\textbf{x})$做最小化. 因此我们重写这个误差函数:$$E =\sum_{n=1}^{N}exp\{-t_{n}f_{m-1}(\textbf x_n)-\frac{1}{2}t_{n}\alpha_{m}y_{m}(\textbf x_n)\} \=\sum_{n=1}^{N}w_{n}^{(m)}exp\{-\frac{1}{2}t_{n}\alpha_{m}y_{m}(\textbf x_n)\}$$其中在我们仅对$\alpha_{m}$ and $y_{m}(\textbf{x})$ 做最优化, 所以$w_{n}^{(m)}=exp\{-t_{n}f_{m-1}(\textbf x_n)\}$可以被认为是常数. 如果我们用$\Gamma_{m}$代表被 $y_{m}(\textbf{x})$ 分对的样本点, $\mathcal M_m$ 代表分错的样本点,那么我们可以把错误方差能写成如下的形式$$\begin{align}E &amp; = e^{-\alpha_{m}/2}\sum_{n\in \Gamma_{m}}w_{n}^{(m)}+e^{\alpha_{m}/2}\sum_{n\in \mathcal M_m}w_{n}^{(m)} \\&amp; = e^{-\alpha_m/2}\sum_{n\in \Gamma_{m}}w_{n}^{(m)}+e^{-\alpha_{m}/2}\sum_{n\in \mathcal M_m}w_{n}^{(m)}+(e^{\alpha_{m}/2}-e^{-\alpha_{m}/2})\sum_{n\in \mathcal M_m}w_{n}^{(m)} \\&amp; = e^{-\alpha_{m}/2}\sum_{n=1}^Nw_{n}^{(m)}+(e^{\alpha_{m}/2}-e^{-\alpha_{m}/2})\sum_{n=1}^Nw_{n}^{(m)}I(y_{m}(\textbf x_n)\neq t_{n})\end{align}$$当我们对$y_{m}(\textbf{x})$求导的时候,第一项是常数,所以对错误方差求最小值就相当于在adaboost算法中最小化$J_m$ ,因为前面的系数不会影响最小值的位置.相似的,我们对$\alpha_{m}$求导,$$\frac{\partial E}{\partial \alpha_{m}} = -\frac{1}{2}e^{-\alpha_{m}/2}\sum_{n=1}^Nw_{n}^{(m)}+\frac{1}{2}(e^{\alpha_{m}/2}+e^{-\alpha_{m}/2})\sum_{n=1}^Nw_{n}^{(m)}I(y_{m}(\textbf x_n))\neq t_{n})=0$$ $$ \epsilon_{m}=\frac{\sum_{n=1}^{N}w_{n}^{(m)}I(y_{m}(\textbf x_n)\neq t_{n})}{\sum_{n=1}^{N}w_{n}^{(m)}} = \frac{e^{-\alpha_{m}/2}}{e^{\alpha_{m}/2}+e^{-\alpha_{m}/2}}$$ $$\frac{1}{\epsilon_{m}}= \frac{e^{\alpha_{m}/2}+e^{-\alpha_{m}/2}}{e^{-\alpha_{m}/2}}=1+e^{\alpha_{m}}$$ $$\alpha_{m}=ln(\frac{1}{\epsilon_{m}}-1)=ln((1-\epsilon_{m})/\epsilon_{m})$$ 与adaboost算法中一致. 由于$w_{n}^{(m)}=exp\{-t_{n}f_{m-1}(\textbf x_n)\}$,所以$$w_{n}^{(m+1)}=w_{n}^{(m)}exp\{-\frac{1}{2}t_{n}\alpha_{m}y_{m}(\textbf x_n)\}$$由于$$t_{n}y_{m}(\textbf x_n) = 1 - 2I(y_{m}(\textbf x_n)\neq t_{n})$$所以$$\begin{align} w_{n}^{(m+1)}&amp;=w_{n}^{(m)}exp\{-\frac{1}{2}t_{n}\alpha_{m}y_{m}(\textbf x_n)\} \\ &amp;= w_{n}^{(m)}exp\{-\frac{1}{2}\alpha_{m}(1 - 2I(y_{m}(\text x_n)\neq t_{n}) \\ &amp;=w_{n}^{(m)}exp\{-\frac{1}{2}\alpha_{m}\}exp\{\alpha_{m}I(y_{m}(\textbf x_n)\neq t_{n})\}\end{align}$$因为$exp\{-\frac{1}{2}\alpha_{m}\}$与n无关,每次更新的时候都是个常数,所以可以删除.$$w_{n}^{(m+1)}=w_{n}^{(m)}exp\{\alpha_{m}I(y_{m}(\textbf x_n)\neq t_{n})\}$$根据$f_{m}(\textbf x)=\frac{1}{2}\sum_{l=1}^{m}\alpha_{l}y_{l}(\textbf x)$的符号,我们可以得到与adaboost一致的结果,系数$\frac{1}{2}$可以舍掉. 基础Adaboost算法实践 data $X \in \mathbb{R}^{n\times p}$ label $y\in\{-1, +1\}^{n}$ weak learner 是 decision stump. $a\in \mathbb{R}$, $j\in \{1, …, p\}$, $d\in \{-1, +1\}$. 其中 $\textbf{x}\in \mathbb{R}^{p}$ 是个向量,$x_{j}$ 是第$j$个元素. 数据地址:ada_data.mat 我们有1000个训练数据,每一个训练数据有25个特征,以及相应的一个label. 首先初始化weight,$\{\textbf x_i, y_i, w_i\}_{i=1}^n, w_i \geq 0,\sum_{i=1}^nw_i=1$ 然后我们在 decision_stump.m 中返回decision stump的参数,这个参数是最小化加权的错误率$l(a^{\star}, d^{\star}, j^{\star})=min_{a, d, j}l(a, d, j)=min_{a, d, j}\sum_{i=1}^{n}w_{i}1\{h_{a, d, j}(\textbf x_i)\neq y_{i}\}$. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354function [k, a, d] = decision_stump(X, y, w)% decision_stump returns a rule ...% h(x) = d if x(k) ‚, ‚ otherwise,%% Input% X : n * p matrix, each row a sample% y : n * 1 vector, each row a label% w : n * 1 vector, each row a weight%% Output% k : the optimal dimension% a : the optimal threshold% d : the optimal d, 1 or -1% total time complexity required to be O(p*n*logn) or less%%% Your Code Here %%%[m,n] = size(X);minerror = inf;minerror_a = 0;minerror_j = -1;minerror_d = 0;w = w';for i = 1:n comp = repmat(X(:,i),1,m)'; ori_rep = repmat(X(:,i),1,m); result_gt = double(ones(m,m)); result_gt((ori_rep - comp)&gt;=0) = -1.0; label_gt = repmat(y,1,m); [min_value_gt,argmin_gt] = min(w*(result_gt ~= label_gt)); result_lt = double(ones(m,m)); result_lt((ori_rep - comp)&lt;=0) = -1.0; label_lt = repmat(y,1,m); [min_value_lt,argmin_lt] = min(w*(result_lt ~= label_lt)); if min_value_lt &lt; min_value_gt final_error = min_value_lt; final_a = X(argmin_lt,i); final_d = -1.0; else final_error = min_value_gt; final_a = X(argmin_gt,i); final_d = 1.0; end if minerror &gt; final_error minerror = final_error minerror_a = final_a; minerror_d = final_d; minerror_j = i; endendk = minerror_j;a = minerror_a;d = minerror_d;end decision_stump_error.m update_weights.m adaboost_error.m adaboost.m 1234567891011121314151617181920212223242526function w_update = update_weights(X, y, k, a, d, w, alpha)% update_weights update the weights with the recent classifier% % Input% X : n * p matrix, each row a sample% y : n * 1 vector, each row a label% k : selected dimension of features% a : selected threshold for feature-k% d : 1 or -1% w : n * 1 vector, old weights% alpha : weights of the classifiers%% Output% w_update : n * 1 vector, the updated weights%%% Your Code Here %%%p = ((X(:, k) &lt;= a) - 0.5) * 2 * d;result = p;w_update = w.*exp(alpha * (result ~= y));%% 注意要normalization 保证公式分母是一样的w_update = w_update/sum(w_update);%%% Your code Here %%%end ​]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Boost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo-note]]></title>
    <url>%2F2018%2F06%2F01%2Fhexo-note%2F</url>
    <content type="text"><![CDATA[mathjax 不能可以使用bold利用textbf代替 mathjax 不能转译\{,\}导致没有办法输出大括号 编辑node_modules\marked\lib\marked.js 脚本， 将464 escape: /^\(\`*{}[\# +-.!_&gt;])/, 替换为 escape: /^\(\`*[\# +-.!_&gt;])/, 这一步取消了对\{,\}的转义(escape)]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
